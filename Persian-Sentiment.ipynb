{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connect to Google colab \n",
        "<a href=\"https://colab.research.google.com/github/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/blob/main/Persian-Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sMNPBU2OwSBw"
      },
      "source": [
        "# Persian Sentiment Analysis With Fasttext language Model and LSTM neural network\n",
        "### Persian sentiment analysis step by step guide\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "so there are 5 steps we going through with each other "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nS1VtmSF6wmn"
      },
      "source": [
        "## Step 1)Choose and Preparing word embedding model\n",
        "in this step we gonna to prepare [word embedding](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) model.\n",
        "there are too many ways to train a word embedding model for example :\n",
        "\n",
        "1.   Fasttext\n",
        "2.   ELMo (Embeddings from Language Models)\n",
        "3.   Universal Sentence Encoder \n",
        "4.   Word2Vec\n",
        "5.   GloVe (Global Vector)\n",
        "\n",
        "if you Want to know more then read [this article from Thomas Wolf](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a) but now we gonna use Fasttext because it's Pretrained by Facebook and we can use it ( there is nothing to worry about this model it's pretty easy to train it by your self or your corpus facebook used Persian Wikipedia and some other staff as dataset for this model so it's just very simpler for us)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "BELKe6-qixIA"
      },
      "outputs": [],
      "source": [
        "#@title Download, extract and load Fasttext 2016 word embedding model\n",
        "# There are also newer models of fasttext in Persian language\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gunzip /content/cc.fa.300.bin.gz\n",
        "\n",
        "!pip install pybind11==2.11.1\n",
        "!pip install fasttext==0.9.2 \n",
        "\n",
        "import fasttext \n",
        "\n",
        "%time\n",
        "fasttext_model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test fasttext model whit word similarity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title test Fasttext word embedding model by similar word\n",
        "\n",
        "phrase = \"\\u062A\\u062A\\u0648\" #@param {type:\"string\"}\n",
        "print(\"dimension of \" + phrase + \" is \" +str(fasttext_model.get_dimension()))\n",
        "print(fasttext_model.get_word_vector(phrase).shape)\n",
        "print(fasttext_model[phrase]) # get the vector of the word \n",
        "\n",
        "# get similar word\n",
        "fasttext_model.get_nearest_neighbors(phrase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ZKbBDbX7Mza"
      },
      "source": [
        "## Step 2) Normalization and preparation of data sets\n",
        "in this step we going to collect a dataset that crawled by [@minasmz](https://github.com/minasmz) it's not good and I only used 450 pos and 450 neg reviews from it.anyway here we will download the dataset and split it to train and test ( I created Train and Test then I filled it with data )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "nAQMVT05MMY4"
      },
      "outputs": [],
      "source": [
        "#@title Upload in google colab and prepare Dataset\n",
        "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/sentiment_tagged_dataset.csv\n",
        "\n",
        "!pip install pandas==1.5.3\n",
        "!pip install numpy==1.23\n",
        "!pip install hazm==0.7.0\n",
        "\n",
        "import pandas\n",
        "import random\n",
        "import numpy\n",
        "import hazm\n",
        "\n",
        "# load and read sentiment_tagged dataset.csv file in tne path ./content/ in google colab. \n",
        "# this dataset include three element: Comment,Score,Suggestion. Comment is feature and Suggestion is label.\n",
        "csv_dataset = pandas.read_csv(\"/content/sentiment_tagged_dataset.csv\")\n",
        "\n",
        "def CleanPersianText(text):\n",
        "  _normalizer = hazm.Normalizer()\n",
        "  text = _normalizer.normalize(text)\n",
        "  return text\n",
        "\n",
        "# Cleansing the dataset and creating a new list with two elements: \"Comment\" and \"suggestion\"filde. (but without the third element: \"score\")\n",
        "# The new list is created by the zip command --> x= zip(csv_dataset['Text'],csv_dataset['Suggestion'])\n",
        "# valu of suggestion is 1,2,3 or positive,negative,neutral\n",
        "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Comment'],csv_dataset['Suggestion'])))\n",
        "\n",
        "# Separation of positive and negative suggestions\n",
        "positive=list(filter(lambda x: x[1] == 1,revlist))\n",
        "neutral=list(filter(lambda x: x[1] == 2,revlist))\n",
        "negative=list(filter(lambda x: x[1] == 3,revlist))\n",
        "\n",
        "# print number of element exist in positive, neutral, negative, revlist list \n",
        "print(\"*\" * 88)\n",
        "print(\"Posetive count {}\".format(len(positive)))\n",
        "print(\"*Negetive count {}\".format(len(negative)))\n",
        "print(\"Natural  count {}\".format(len(neutral)))\n",
        "print(\"Total dataset count {}\".format(len(revlist)))\n",
        "\n",
        "# mix positive and negative suggestions for 450 element.\n",
        "# We chose 450 because the most negative comments were 450\n",
        "revlist_shuffle = positive[:460] + negative[:460]\n",
        "random.shuffle(revlist_shuffle)\n",
        "random.shuffle(revlist_shuffle)#double shuffle\n",
        "print(\"Total shuffle count {}\".format(len(revlist_shuffle)),\"\\n\")\n",
        "\n",
        "# print random element from positive, neutral, negative List\n",
        "print(\"Random Posetive Comment: \",\"\\n\",positive[random.randrange(1,len(positive))])\n",
        "print(\"Random Negetive Comment: \",\"\\n\",negative[random.randrange(1,len(negative))])\n",
        "print(\"Random unknown  Comment: \",\"\\n\",neutral[random.randrange(1,len(neutral))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "wUJceKehjfJ3"
      },
      "outputs": [],
      "source": [
        "#@title create and Prepare Train & Test data_structure with zero value\n",
        "embedding_dim = 300 #@param {type:\"integer\"}\n",
        "max_vocab_token = 20 #@param {type:\"integer\"}\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "\n",
        "train_size = int(0.95*(len(revlist_shuffle)))\n",
        "test_size = int(0.05*(len(revlist_shuffle)))\n",
        "\n",
        "# x_train same as features and y_train same as the label. x_train same as input and y_train same as output.\n",
        "# The x_train data have 3 Dimention (874,20,300): (number_of_comment,number_of_words, dimension_of_fasttext)\n",
        "# The y_train data has 2 dimensions (874,2): (number of comments, suggestions)\n",
        "# The suggestions are 1 or 3. 1's are positive and 3's are negative suggestions.\n",
        "x_train = np.zeros((train_size, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
        "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
        "\n",
        "x_test = np.zeros((test_size, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
        "y_test = np.zeros((test_size, 2), dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "jvcGBpjPwFL0"
      },
      "outputs": [],
      "source": [
        "#@title Fill X_Train, X_Test, Y_Train, Y_Test with digi-kala Dataset\n",
        "indexes = set(np.random.choice(len(revlist_shuffle), train_size + test_size, replace=False)) # for random selection\n",
        "print(\"data_item is: \" + str(len(indexes)),\"\\n\")\n",
        "\n",
        "for data_item, index in enumerate(indexes): # indexes include 920 items of comments\n",
        "  comment = hazm.word_tokenize(revlist_shuffle[index][0]) #[0] means the \"comment\" field in the .csv file\n",
        "  for vocabs in range(0,len(comment)):\n",
        "    if vocabs >= max_vocab_token: \n",
        "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
        "    if comment[vocabs] not in fasttext_model.words:\n",
        "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector in x_train is zero\n",
        "    if data_item < train_size:\n",
        "      x_train[data_item, vocabs, :] = fasttext_model.get_word_vector(comment[vocabs])\n",
        "    else:\n",
        "      x_test[data_item - train_size, vocabs, :] = fasttext_model.get_word_vector(comment[vocabs])\n",
        "\n",
        "  if data_item < train_size:\n",
        "    y_train[data_item, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "  else:\n",
        "    y_test[data_item - train_size, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "    \n",
        "print (x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dDunM15J7n8E"
      },
      "source": [
        "## Step 3) Config & Compile & fit the LSTM model\n",
        "Now we will create our LSTM model then feed it our Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "_Mbfwpab3Yb8"
      },
      "outputs": [],
      "source": [
        "#@title Set batchSize and epochs\n",
        "batch_size = 500 #@param {type:\"integer\"}\n",
        "no_epochs = 150 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code will help you build a neural network model with LSTM, which is capable of predicting the level of delusion, i.e. the dangerousness of an opinion.\n",
        "\n",
        "First, we create the LSTM_model and add layers to it sequentially.\n",
        "\n",
        "First, a Conv1D layer is added to the model, which is used to convert each word into a suitable vector.\n",
        "\n",
        "In this model, two more Conv1D layers have been added to the model, which use 3x3 size filters.\n",
        "\n",
        "A MaxPooling1D layer with a window size of 3 is also added to the model because it helps reduce dimensionality (i.e. ease of processing).\n",
        "\n",
        "Then an LSTM layer with 512 neurons is added to the model, which uses long sentences for prediction.\n",
        "\n",
        "Then three perceptron layers with sigmoid activations are added to the model. The dimensions of these layers are 512, 512 and 512 respectively.\n",
        "\n",
        "To prevent overfitting, three Dropout layers with coefficients of 0.2 and 0.25 are used.\n",
        "\n",
        "Finally, a Dense layer is added to the model which is the number of desired decision output (in this case 2) and finally softmax is used as activation which returns the probabilities of the classes.\n",
        "\n",
        "The compile function is used to set the parameters of the model, where categorical_crossentropy is used as a loss function and is used for Adam optimization.\n",
        "\n",
        "At the end, by using model print, we get a summary of the model structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "J1z_mq913jTq"
      },
      "outputs": [],
      "source": [
        "#@title Building Layers of LSTM Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "\n",
        "LSTM_model = Sequential() \n",
        "\n",
        "# Firstly, we will add an embedding layer which will convert each word into vector & set the hyperparameters of the layer\n",
        "# We use Conv1D because sentences have one dimension: Convolutional layer is 20x300 and filter(kernel_size)=32 3x3\n",
        "LSTM_model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_vocab_token, embedding_dim)))\n",
        "\n",
        "LSTM_model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
        "LSTM_model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "LSTM_model.add(MaxPooling1D(pool_size=3)) # Down sampling\n",
        "\n",
        "# Add LSTM layer whit 512 neron & Dropout--> use for prevent of overfitting\n",
        "LSTM_model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))\n",
        "\n",
        "# \"Dense\" refers to a fully connected layer\n",
        "LSTM_model.add(Dense(512, activation='sigmoid')) # sigmoid --> use for binary classification\n",
        "LSTM_model.add(Dropout(0.2)) # Dropout--> use for prevent of overfitting\n",
        "LSTM_model.add(Dense(512, activation='sigmoid'))\n",
        "LSTM_model.add(Dropout(0.25))\n",
        "LSTM_model.add(Dense(512, activation='sigmoid'))\n",
        "LSTM_model.add(Dropout(0.25))\n",
        "\n",
        "# Dense 2 --> this layer is used to Decision between two classes.\n",
        "LSTM_model.add(Dense(2, activation='softmax')) # softmax --> Returns the probability of a comment for each class.\n",
        "\n",
        "# categorical_crossentropy cost function is used for multi-category classification problems.\n",
        "# Adam's optimization algorithm is used and lr=0.0001 determine the learning rate and decay=1e-6 determine step size reduction rate\n",
        "LSTM_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
        "\n",
        "# Show Dashboard\n",
        "#tensorboard = TensorBoard(log_dir='logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "print(LSTM_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title start learning\n",
        "\n",
        "LSTM_model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4) Evaluate and Save our model\n",
        "in this step we evaluate LSTM model loss and accuracy metric\n",
        "loss: 0.5849 - accuracy: 0.8333"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3TKsHaoO7HpW"
      },
      "outputs": [],
      "source": [
        "LSTM_model.metrics_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UpocYJkB7KMs"
      },
      "outputs": [],
      "source": [
        "# model evaluate\n",
        "LSTM_model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U5xPkhZX7Qmq"
      },
      "outputs": [],
      "source": [
        "# Save the model for future use\n",
        "LSTM_model.save('learned-persian-sentiment-fasttext.model') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJxZj2vr7uMO"
      },
      "source": [
        "# Step 5) Test our model\n",
        "there is two form but it's just for showcase there is no diff between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "xXt5rQ0qmyax"
      },
      "outputs": [],
      "source": [
        "#@title using model\n",
        "\n",
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  normal_text = _normalizer.normalize(user_text)\n",
        "  tokenized_text = hazm.word_tokenize(normal_text)\n",
        "  \n",
        "  # create and Prepare three dimension tensor (1,20,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
        "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
        "\n",
        "\n",
        "  for vocabs in range(0,len(tokenized_text)):\n",
        "    if vocabs >= max_vocab_token:\n",
        "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
        "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
        "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
        "    \n",
        "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
        "\n",
        "  # print(vector_text.shape)\n",
        "  # print(vector_text)\n",
        "  result = LSTM_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % üòç\"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % ü§ï\"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "_kLhLv1h7UD_"
      },
      "outputs": [],
      "source": [
        "#@title using model\n",
        "\n",
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  normal_text = _normalizer.normalize(user_text)\n",
        "  tokenized_text = hazm.word_tokenize(normal_text)\n",
        "  \n",
        "  # create and Prepare three dimension tensor (1,20,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
        "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
        "\n",
        "\n",
        "  for vocabs in range(0,len(tokenized_text)):\n",
        "    if vocabs >= max_vocab_token:\n",
        "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
        "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
        "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
        "    \n",
        "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
        "\n",
        "  # print(vector_text.shape)\n",
        "  # print(vector_text)\n",
        "  result = LSTM_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % üòç\"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % ü§ï\"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u062C\\u0627\\u0644\\u0628\\u0647 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0627\\u0635\\u0644\\u0627 \\u0647\\u0645\\u0647 \\u0686\\u06CC \\u062A\\u0645\\u0627\\u0645\\u0647 \\u0645\\u0646 \\u06A9\\u0647 \\u067E\\u0633\\u0646\\u062F\\u06CC\\u062F\\u0645 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0632\\u06CC\\u0628\\u0627 \\u0631\\u0648\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  normal_text = _normalizer.normalize(user_text)\n",
        "  tokenized_text = hazm.word_tokenize(normal_text)\n",
        "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
        "  for vocabs in range(0,len(tokenized_text)):\n",
        "    if vocabs >= max_vocab_token:\n",
        "      break\n",
        "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
        "      continue\n",
        "    \n",
        "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = LSTM_model.predict(vector_text)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
