{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connect to Google colab \n",
        "<a href=\"https://colab.research.google.com/github/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/blob/main/Persian-Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sMNPBU2OwSBw"
      },
      "source": [
        "# Persian Sentiment Analysis With Fasttext language Model and LSTM neural network\n",
        "### Persian sentiment analysis step by step guide\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### so there are 5 steps we going through with each other "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1) Choose and Preparing Word Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nS1VtmSF6wmn"
      },
      "source": [
        "in this step we gonna to prepare word embedding model.(https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) \n",
        "there are too many ways to train a word embedding model for example :\n",
        "\n",
        "1.   Fasttext\n",
        "2.   ELMo (Embeddings from Language Models)\n",
        "3.   Universal Sentence Encoder \n",
        "4.   Word2Vec\n",
        "5.   GloVe (Global Vector)\n",
        "\n",
        "if you Want to know more then read [this article from Thomas Wolf](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a) but now we gonna use Fasttext because it's Pretrained by Facebook and we can use it ( there is nothing to worry about this model it's pretty easy to train it by your self or your corpus facebook used Persian Wikipedia and some other staff as dataset for this model so it's just very simpler for us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "BELKe6-qixIA"
      },
      "outputs": [],
      "source": [
        "#@title Download, extract and load Fasttext 2016 word embedding model\n",
        "# There are also newer models of fasttext in Persian language\n",
        "\n",
        "!pip install pybind11==2.11.1\n",
        "!pip install fasttext==0.9.2\n",
        "\n",
        "!pip install keras==2.12.0 \n",
        "!pip install pandas==1.5.3\n",
        "!pip install numpy==1.23\n",
        "!pip install hazm==0.7.0\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gunzip /content/cc.fa.300.bin.gz\n",
        "\n",
        "import fasttext \n",
        "\n",
        "%time\n",
        "fasttext_model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title test Fasttext word embedding model by similar word\n",
        "\n",
        "phrase = \"\\u062A\\u062A\\u0648\" #@param {type:\"string\"}\n",
        "print(\"dimension of \" + phrase + \" is \" +str(fasttext_model.get_dimension()))\n",
        "print(fasttext_model.get_word_vector(phrase).shape)\n",
        "print(fasttext_model[phrase]) # get the vector of the word \n",
        "\n",
        "# get similar word\n",
        "fasttext_model.get_nearest_neighbors(phrase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2) Normalization and Preparation of Data Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ZKbBDbX7Mza"
      },
      "source": [
        "in this step we going to collect a dataset that crawled by [@minasmz](https://github.com/minasmz) it's not good and I only used 450 pos and 450 neg reviews from it.anyway here we will download the dataset and split it to train and test ( I created Train and Test then I filled it with data )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "nAQMVT05MMY4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas==1.5.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (1.24.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.23"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/03/c6/14a17e10813b8db20d1e800ff9a3a898e65d25f2b0e9d6a94616f1e3362c/numpy-1.23.0.tar.gz\n",
            "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/03/c6/14a17e10813b8db20d1e800ff9a3a898e65d25f2b0e9d6a94616f1e3362c/numpy-1.23.0.tar.gz\n",
            "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/03/c6/14a17e10813b8db20d1e800ff9a3a898e65d25f2b0e9d6a94616f1e3362c/numpy-1.23.0.tar.gz\n",
            "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/03/c6/14a17e10813b8db20d1e800ff9a3a898e65d25f2b0e9d6a94616f1e3362c/numpy-1.23.0.tar.gz\n",
            "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/03/c6/14a17e10813b8db20d1e800ff9a3a898e65d25f2b0e9d6a94616f1e3362c/numpy-1.23.0.tar.gz\n",
            "ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/03/c6/14a17e10813b8db20d1e800ff9a3a898e65d25f2b0e9d6a94616f1e3362c/numpy-1.23.0.tar.gz (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\"))\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm==0.7.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from hazm==0.7.0) (3.3)\n",
            "Requirement already satisfied: six in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.3->hazm==0.7.0) (1.16.0)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'inspect' has no attribute 'formatargspec'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhazm\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# load and read sentiment_tagged dataset.csv file in tne path ./content/ in google colab. \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# this dataset include three element: Comment,Score,Suggestion. Comment is feature and Suggestion is label.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m csv_dataset \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/sentiment_tagged_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hazm\\__init__.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWordTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTokenSplitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenSplitter\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hazm\\WordTokenizer.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcodecs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m words_list, default_words, default_verbs\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenizerI\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWordTokenizer\u001b[39;00m(TokenizerI):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\t>>> tokenizer = WordTokenizer()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\t>>> tokenizer.tokenize('این جمله (خیلی) پیچیده نیست!!!')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\tدیگه میخوام ترک تحصیل کنم 😂 😂 😂\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\__init__.py:115\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrammar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:187\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(obj, name, default)\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;129;43m@decorator\u001b[39;49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmemoize\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgetattr_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemoize_dic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# memoize_dic is created at the first call\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:176\u001b[0m, in \u001b[0;36mdecorator\u001b[1;34m(caller)\u001b[0m\n\u001b[0;32m    174\u001b[0m     dec_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(src, \u001b[38;5;28mdict\u001b[39m(_func_\u001b[38;5;241m=\u001b[39mfunc, _call_\u001b[38;5;241m=\u001b[39mcaller))\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_wrapper(dec_func, func, infodict)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_decorator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:87\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[1;34m(wrapper, model, infodict)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_wrapper\u001b[39m(wrapper, model, infodict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 87\u001b[0m     infodict \u001b[38;5;241m=\u001b[39m infodict \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m infodict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     89\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m infodict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:69\u001b[0m, in \u001b[0;36mgetinfo\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m varkwargs:\n\u001b[0;32m     68\u001b[0m     argnames\u001b[38;5;241m.\u001b[39mappend(varkwargs)\n\u001b[1;32m---> 69\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatargspec\u001b[49m(regargs, varargs, varkwargs, defaults,\n\u001b[0;32m     70\u001b[0m                                   formatvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m value: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# pypy compatibility\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'inspect' has no attribute 'formatargspec'"
          ]
        }
      ],
      "source": [
        "#@title Upload in google colab and prepare Dataset\n",
        "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/sentiment_tagged_dataset.csv\n",
        "\n",
        "import pandas\n",
        "import random\n",
        "import numpy\n",
        "import hazm\n",
        "\n",
        "# load and read sentiment_tagged dataset.csv file in tne path ./content/ in google colab. \n",
        "# this dataset include three element: Comment,Score,Suggestion. Comment is feature and Suggestion is label.\n",
        "csv_dataset = pandas.read_csv(\"/content/sentiment_tagged_dataset.csv\")\n",
        "\n",
        "def CleanPersianText(text):\n",
        "  _normalizer = hazm.Normalizer()\n",
        "  text = _normalizer.normalize(text)\n",
        "  return text\n",
        "\n",
        "# Cleansing the dataset and creating a new list with two elements: \"Comment\" and \"suggestion\"filde. (but without the third element: \"score\")\n",
        "# The new list is created by the zip command --> x= zip(csv_dataset['Comment'],csv_dataset['Suggestion'])\n",
        "# valu of suggestion is 1,2,3 or positive,negative,neutral\n",
        "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Comment'],csv_dataset['Suggestion'])))\n",
        "\n",
        "# Separation of positive and negative suggestions\n",
        "positive=list(filter(lambda x: x[1] == 1,revlist))\n",
        "neutral=list(filter(lambda x: x[1] == 2,revlist))\n",
        "negative=list(filter(lambda x: x[1] == 3,revlist))\n",
        "\n",
        "# print number of element exist in positive, neutral, negative, revlist list \n",
        "print(\"*\" * 88)\n",
        "print(\"Posetive count {}\".format(len(positive)))\n",
        "print(\"*Negetive count {}\".format(len(negative)))\n",
        "print(\"Natural  count {}\".format(len(neutral)))\n",
        "print(\"Total dataset count {}\".format(len(revlist)))\n",
        "\n",
        "# mix positive and negative suggestions for 460 element.\n",
        "# We chose 460 because the most negative comments were 460\n",
        "revlist_shuffle = positive[:460] + negative[:460]\n",
        "random.shuffle(revlist_shuffle)\n",
        "random.shuffle(revlist_shuffle)#double shuffle\n",
        "print(\"Total shuffle count {}\".format(len(revlist_shuffle)),\"\\n\")\n",
        "\n",
        "# print random element from positive, neutral, negative List\n",
        "print(\"Random Posetive Comment: \",\"\\n\",positive[random.randrange(1,len(positive))])\n",
        "print(\"Random Negetive Comment: \",\"\\n\",negative[random.randrange(1,len(negative))])\n",
        "print(\"Random unknown  Comment: \",\"\\n\",neutral[random.randrange(1,len(neutral))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "wUJceKehjfJ3"
      },
      "outputs": [],
      "source": [
        "#@title create and Prepare Train & Test data_structure with zero value\n",
        "embedding_dim = 300 #@param {type:\"integer\"}\n",
        "max_vocab_token = 20 #@param {type:\"integer\"}\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "\n",
        "train_size = int(0.95*(len(revlist_shuffle)))\n",
        "test_size = int(0.05*(len(revlist_shuffle)))\n",
        "\n",
        "# x_train same as features and y_train same as the label. x_train same as input and y_train same as output.\n",
        "# The x_train data have 3 Dimention (874,20,300): (number_of_comment,number_of_words, dimension_of_fasttext)\n",
        "# The y_train data has 2 dimensions (874,2): (number of comments, suggestions)\n",
        "# The suggestions are 1 or 3. 1's are positive and 3's are negative suggestions.\n",
        "x_train = np.zeros((train_size, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
        "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
        "\n",
        "x_test = np.zeros((test_size, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
        "y_test = np.zeros((test_size, 2), dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "jvcGBpjPwFL0"
      },
      "outputs": [],
      "source": [
        "#@title Fill X_Train, X_Test, Y_Train, Y_Test with digi-kala Dataset\n",
        "indexes = set(np.random.choice(len(revlist_shuffle), train_size + test_size, replace=False)) # for random selection\n",
        "print(\"data_item is: \" + str(len(indexes)),\"\\n\")\n",
        "\n",
        "for data_item, index in enumerate(indexes): # indexes include 920 items of comments\n",
        "  comment = hazm.word_tokenize(revlist_shuffle[index][0]) #[0] means the \"comment\" field in the .csv file\n",
        "  for vocabs in range(0,len(comment)):\n",
        "    if vocabs >= max_vocab_token: \n",
        "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
        "    if comment[vocabs] not in fasttext_model.words:\n",
        "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector in x_train is zero\n",
        "    if data_item < train_size:\n",
        "      x_train[data_item, vocabs, :] = fasttext_model.get_word_vector(comment[vocabs])\n",
        "    else:\n",
        "      x_test[data_item - train_size, vocabs, :] = fasttext_model.get_word_vector(comment[vocabs])\n",
        "\n",
        "  if data_item < train_size:\n",
        "    y_train[data_item, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "  else:\n",
        "    y_test[data_item - train_size, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "    \n",
        "print (x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3) Config & Compile & Fit the LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dDunM15J7n8E"
      },
      "source": [
        "Now we will create our LSTM model then feed it our Train data\n",
        "\n",
        "This code will help you build a neural network model with LSTM, which is capable of predicting the level of delusion, i.e. the dangerousness of an opinion.\n",
        "First, we create the LSTM_model and add layers to it sequentially.\n",
        "First, a Conv1D layer is added to the model, which is used to convert each word into a suitable vector.\n",
        "In this model, two more Conv1D layers have been added to the model, which use 3x3 size filters.\n",
        "A MaxPooling1D layer with a window size of 3 is also added to the model because it helps reduce dimensionality (i.e. ease of processing).\n",
        "Then an LSTM layer with 512 neurons is added to the model, which uses long sentences for prediction.\n",
        "Then three perceptron layers with sigmoid activations are added to the model. The dimensions of these layers are 512, 512 and 512 respectively.\n",
        "To prevent overfitting, three Dropout layers with coefficients of 0.2 and 0.25 are used.\n",
        "Finally, a Dense layer is added to the model which is the number of desired decision output (in this case 2) and finally softmax is used as activation which returns the probabilities of the classes.\n",
        "The compile function is used to set the parameters of the model, where categorical_crossentropy is used as a loss function and is used for Adam optimization.\n",
        "At the end, by using model print, we get a summary of the model structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "_Mbfwpab3Yb8"
      },
      "outputs": [],
      "source": [
        "#@title Set batchSize and epochs\n",
        "# batch_size: is the number of data to be selected in each step\n",
        "batch_size = 500 #@param {type:\"integer\"}\n",
        "no_epochs = 200 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "J1z_mq913jTq"
      },
      "outputs": [],
      "source": [
        "#@title Building Layers of LSTM Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "\n",
        "LSTM_model = Sequential() \n",
        "\n",
        "# Firstly, we will add an embedding layer which will convert each word into vector & set the hyperparameters of the layer\n",
        "# We use Conv1D because sentences have one dimension: Convolutional layer is 20x300 and filter(kernel_size)=32 3x3\n",
        "LSTM_model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_vocab_token, embedding_dim)))\n",
        "\n",
        "LSTM_model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
        "LSTM_model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "LSTM_model.add(MaxPooling1D(pool_size=3)) # Down sampling\n",
        "\n",
        "# Add LSTM layer whit 512 neron & Dropout--> use for prevent of overfitting\n",
        "LSTM_model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))\n",
        "\n",
        "# \"Dense\" refers to a fully connected layer\n",
        "LSTM_model.add(Dense(512, activation='sigmoid')) # sigmoid --> use for binary classification\n",
        "LSTM_model.add(Dropout(0.2)) # Dropout--> use for prevent of overfitting\n",
        "LSTM_model.add(Dense(512, activation='sigmoid'))\n",
        "LSTM_model.add(Dropout(0.25))\n",
        "LSTM_model.add(Dense(512, activation='sigmoid'))\n",
        "LSTM_model.add(Dropout(0.25))\n",
        "\n",
        "# Dense 2 --> this layer is used to Decision between two classes.\n",
        "LSTM_model.add(Dense(2, activation='softmax')) # softmax --> Returns the probability of a comment for each class.\n",
        "\n",
        "# categorical_crossentropy cost function is used for multi-category classification problems.\n",
        "# Adam's optimization algorithm is used and lr=0.0001 determine the learning rate and decay=1e-6 determine step size reduction rate\n",
        "LSTM_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
        "\n",
        "# Show Dashboard\n",
        "#tensorboard = TensorBoard(log_dir='logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "print(LSTM_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title start learning\n",
        "\n",
        "LSTM_model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4) Evaluate and Save our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in this step we evaluate LSTM model loss and accuracy metric\n",
        "loss: 0.5849 - accuracy: 0.8333"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3TKsHaoO7HpW"
      },
      "outputs": [],
      "source": [
        "LSTM_model.metrics_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UpocYJkB7KMs"
      },
      "outputs": [],
      "source": [
        "# model evaluate\n",
        "LSTM_model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U5xPkhZX7Qmq"
      },
      "outputs": [],
      "source": [
        "# Save the model for future use\n",
        "LSTM_model.save('learned-persian-sentiment-fasttext.model') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5) Test our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJxZj2vr7uMO"
      },
      "source": [
        "###there is three form but it's just for showcase there is no diff between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "xXt5rQ0qmyax"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21964\\2380458856.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
            "  from IPython.core.display import display, HTML\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'hazm' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m user_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\u062E\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u0644\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u06AF\u001b[39;00m\u001b[38;5;130;01m\\u0648\u001b[39;00m\u001b[38;5;130;01m\\u0634\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u062E\u001b[39;00m\u001b[38;5;130;01m\\u0648\u001b[39;00m\u001b[38;5;130;01m\\u0628\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\u062A\u001b[39;00m\u001b[38;5;130;01m\\u0634\u001b[39;00m\u001b[38;5;130;01m\\u062E\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u0635\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0686\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;130;01m\\u0631\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u062F\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u0631\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\u062F\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u062E\u001b[39;00m\u001b[38;5;130;01m\\u0644\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u062C\u001b[39;00m\u001b[38;5;130;01m\\u0639\u001b[39;00m\u001b[38;5;130;01m\\u0628\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u06A9\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u0648\u001b[39;00m\u001b[38;5;130;01m\\u0631\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u06AF\u001b[39;00m\u001b[38;5;130;01m\\u0648\u001b[39;00m\u001b[38;5;130;01m\\u0634\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0648\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0645\u001b[39;00m\u001b[38;5;130;01m\\u062D\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u0641\u001b[39;00m\u001b[38;5;130;01m\\u0638\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0635\u001b[39;00m\u001b[38;5;130;01m\\u0641\u001b[39;00m\u001b[38;5;130;01m\\u062D\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u062F\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u0631\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\u0645\u001b[39;00m\u001b[38;5;130;01m\\u0646\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u062F\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u0631\u001b[39;00m\u001b[38;5;130;01m\\u0648\u001b[39;00m\u001b[38;5;130;01m\\u0632\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0628\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u062F\u001b[39;00m\u001b[38;5;130;01m\\u0633\u001b[39;00m\u001b[38;5;130;01m\\u062A\u001b[39;00m\u001b[38;5;130;01m\\u0645\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0631\u001b[39;00m\u001b[38;5;130;01m\\u0633\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u062F\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0639\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u0644\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u0647\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0645\u001b[39;00m\u001b[38;5;130;01m\\u0631\u001b[39;00m\u001b[38;5;130;01m\\u0633\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u0632\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u062F\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;130;01m\\u062C\u001b[39;00m\u001b[38;5;130;01m\\u06CC\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\u06A9\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;130;01m\\u0644\u001b[39;00m\u001b[38;5;130;01m\\u0627\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#@param {type:\"string\"}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[1;32m----> 5\u001b[0m _normalizer \u001b[38;5;241m=\u001b[39m \u001b[43mhazm\u001b[49m\u001b[38;5;241m.\u001b[39mNormalizer()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_text\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      7\u001b[0m   normal_text \u001b[38;5;241m=\u001b[39m _normalizer\u001b[38;5;241m.\u001b[39mnormalize(user_text)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'hazm' is not defined"
          ]
        }
      ],
      "source": [
        "#@title using model\n",
        "\n",
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  normal_text = _normalizer.normalize(user_text)\n",
        "  tokenized_text = hazm.word_tokenize(normal_text)\n",
        "  \n",
        "  # create and Prepare three dimension tensor (1,20,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
        "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
        "\n",
        "\n",
        "  for vocabs in range(0,len(tokenized_text)):\n",
        "    if vocabs >= max_vocab_token:\n",
        "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
        "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
        "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
        "    \n",
        "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
        "\n",
        "  # print(vector_text.shape)\n",
        "  # print(vector_text)\n",
        "  result = LSTM_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % 😍\"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % 🤕\"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "_kLhLv1h7UD_"
      },
      "outputs": [],
      "source": [
        "#@title using model\n",
        "\n",
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  normal_text = _normalizer.normalize(user_text)\n",
        "  tokenized_text = hazm.word_tokenize(normal_text)\n",
        "  \n",
        "  # create and Prepare three dimension tensor (1,20,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
        "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
        "\n",
        "\n",
        "  for vocabs in range(0,len(tokenized_text)):\n",
        "    if vocabs >= max_vocab_token:\n",
        "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
        "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
        "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
        "    \n",
        "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
        "\n",
        "  # print(vector_text.shape)\n",
        "  # print(vector_text)\n",
        "  result = LSTM_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % 😍\"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % 🤕\"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u062C\\u0627\\u0644\\u0628\\u0647 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0627\\u0635\\u0644\\u0627 \\u0647\\u0645\\u0647 \\u0686\\u06CC \\u062A\\u0645\\u0627\\u0645\\u0647 \\u0645\\u0646 \\u06A9\\u0647 \\u067E\\u0633\\u0646\\u062F\\u06CC\\u062F\\u0645 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0632\\u06CC\\u0628\\u0627 \\u0631\\u0648\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "_normalizer = hazm.Normalizer()\n",
        "if not user_text==\"\":\n",
        "  normal_text = _normalizer.normalize(user_text)\n",
        "  tokenized_text = hazm.word_tokenize(normal_text)\n",
        "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
        "  for vocabs in range(0,len(tokenized_text)):\n",
        "    if vocabs >= max_vocab_token:\n",
        "      break\n",
        "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
        "      continue\n",
        "    \n",
        "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = LSTM_model.predict(vector_text)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
