{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Google colab \n",
    "<a href=\"https://colab.research.google.com/github/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/blob/main/query-Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMNPBU2OwSBw"
   },
   "source": [
    "# Persian Sentiment Analysis With Fasttext language Model and LSTM convolutional neural network(CNN)\n",
    "### Persian sentiment analysis step by step guide\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### so there are 5 steps we going through with each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Choose and Preparing Word Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nS1VtmSF6wmn"
   },
   "source": [
    "in this step we gonna to prepare word embedding model.(https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) \n",
    "there are too many ways to train a word embedding model for example :\n",
    "\n",
    "1.   Fasttext\n",
    "2.   ELMo (Embeddings from Language Models)\n",
    "3.   Universal Sentence Encoder \n",
    "4.   Word2Vec\n",
    "5.   GloVe (Global Vector)\n",
    "\n",
    "if you Want to know more then read [this article from Thomas Wolf](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a) but now we gonna use Fasttext because it's Pretrained by Facebook and we can use it ( there is nothing to worry about this model it's pretty easy to train it by your self or your corpus facebook used Persian Wikipedia and some other staff as dataset for this model so it's just very simpler for us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Python Package and library version\n",
    "\n",
    "pip install pybind11==2.11.1\n",
    "#pip install fasttext==0.9.2 #For Word Embedding Model \n",
    "pip install fasttext-wheel # Alternative For pip install fasttext\n",
    "\n",
    "pip install tensorflow==2.12.0 #For Deep Learning\n",
    "pip install keras==2.12.0 #A wrapper for TensorFlow for simplicity\n",
    "\n",
    "pip install pandas==1.5.3\n",
    "pip install numpy==1.23 \n",
    "pip install hazm==0.7.0 #For NLP processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import random\n",
    "import numpy\n",
    "import hazm \n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "BELKe6-qixIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#@title Download, extract and load Fasttext 2016 word embedding model\n",
    "# There are also newer models of fasttext in Persian language\n",
    "\n",
    "# !rm -rf /content/cc.fa.300.bin.gz\n",
    "# !rm -rf /content/cc.fa.300.bin\n",
    "\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
    "# !gunzip /content/cc.fa.300.bin.gz\n",
    "\n",
    "%time\n",
    "fasttext_model = fasttext.load_model(\"cc.fa.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of تتو is 300\n",
      "(300,)\n",
      "[ 0.01426262  0.08285356  0.05822329  0.00633453  0.1620899   0.13284938\n",
      "  0.04546589  0.11013536 -0.05888556  0.2990161   0.02380084 -0.10438265\n",
      " -0.03706899  0.08250708 -0.01062764 -0.08420737 -0.14434323  0.00297825\n",
      "  0.06046857 -0.24151032 -0.14391941 -0.34899202 -0.2294438  -0.03500568\n",
      "  0.07068318  0.06606355 -0.04481929  0.11544118  0.1107036  -0.13317207\n",
      "  0.02266459  0.09386346  0.08144896 -0.11717169  0.0594755   0.10335512\n",
      "  0.08861537  0.05905498  0.04107049 -0.05282331 -0.2472112  -0.19621985\n",
      " -0.17690527 -0.1552521  -0.16914693 -0.09037551  0.15534705 -0.16592395\n",
      " -0.2956943   0.0527075  -0.1530415   0.07653764 -0.19085737  0.11003933\n",
      " -0.01063968  0.18653959 -0.21423444 -0.17112218  0.12297003 -0.24618636\n",
      "  0.07126046 -0.0327876  -0.21315134 -0.14722669  0.06263144  0.2954651\n",
      "  0.15167138 -0.00172055 -0.03869071 -0.09496982 -0.09250215  0.06288654\n",
      "  0.01881863  0.0531782  -0.04199309  0.11577096 -0.13591702 -0.10669065\n",
      " -0.32872364 -0.17210054 -0.0039242  -0.04442921 -0.13855797  0.1148991\n",
      " -0.07319948 -0.00419761  0.03867396 -0.08836947  0.05686059  0.0168206\n",
      " -0.22696075  0.05936805 -0.02378897  0.1148432  -0.06954018 -0.07233681\n",
      " -0.01874623 -0.23229468 -0.08642146 -0.04949857  0.07823034  0.15526234\n",
      "  0.13322768 -0.08250979 -0.24383318  0.17288911  0.14398853 -0.04701463\n",
      "  0.18383184 -0.01785142 -0.02262333 -0.09288354  0.0424541   0.09701238\n",
      " -0.00321051 -0.00292556  0.07367507  0.09386196 -0.22052118  0.12114282\n",
      " -0.04756912 -0.2657035   0.03476723 -0.08133406  0.04303156  0.21063651\n",
      "  0.07928114  0.34227404 -0.02690347 -0.23240988 -0.04111917  0.23545033\n",
      "  0.16467887  0.09578546  0.09333461  0.3320039   0.01122536  0.04216078\n",
      " -0.19309796  0.13860062  0.09741319  0.00877899  0.14156036 -0.22842434\n",
      "  0.0946355   0.04523856 -0.0931283  -0.02961091 -0.08772214 -0.0113426\n",
      "  0.04781866  0.12302681  0.12595926 -0.16908923  0.12192493 -0.02106297\n",
      "  0.08169825 -0.01404771  0.07327273 -0.16695224  0.05365206 -0.06462354\n",
      " -0.12053522 -0.07944939 -0.12767185  0.02978576  0.06243588 -0.03288365\n",
      " -0.18921691  0.06197268 -0.24800143 -0.10892422  0.11308471  0.02693931\n",
      " -0.18553998 -0.08105126  0.05621798 -0.17978668  0.03781066  0.05664433\n",
      " -0.05755966  0.02747312  0.07800692 -0.26444662 -0.0432292   0.13283579\n",
      "  0.17398077  0.08839136  0.17150678 -0.09937176 -0.02002289  0.17328566\n",
      "  0.08972655  0.13647671 -0.17519593 -0.01415911 -0.13173577 -0.13283587\n",
      " -0.25519335 -0.04303173 -0.15960038  0.15598808  0.07025241  0.00809679\n",
      "  0.0058901   0.07621504  0.03686843  0.12506111 -0.22505935 -0.13575113\n",
      " -0.01808125 -0.0157626   0.00716071  0.09046205  0.03497653  0.10323592\n",
      " -0.00329344  0.0545406   0.10784282  0.04971446  0.13354418 -0.0459842\n",
      "  0.00938249  0.14872831  0.00909764 -0.04132712 -0.11737612 -0.08516882\n",
      " -0.04819297  0.06087807 -0.07225028  0.1373322  -0.01049187  0.03365994\n",
      "  0.02894482  0.02579116  0.04772557  0.005957    0.15669431  0.06006972\n",
      "  0.09709536  0.09381734  0.05959716  0.29758638  0.05378079 -0.02853974\n",
      " -0.2062984   0.07489324 -0.01370804  0.12590134  0.18865977  0.03275551\n",
      " -0.02379034  0.194858   -0.00557285 -0.23484057  0.1586628  -0.00894838\n",
      "  0.00086223  0.11169711 -0.03883936 -0.01760963  0.05675873 -0.02197782\n",
      " -0.05366191 -0.1969426  -0.07178315  0.15878776  0.1908435   0.13278963\n",
      " -0.07821094  0.01168904 -0.14971685 -0.05809875  0.02311807 -0.15522955\n",
      "  0.21519795  0.03036569  0.01384172 -0.12552093  0.09847277  0.01210586\n",
      " -0.00688656 -0.13447666  0.04722724 -0.29562718 -0.01166942 -0.06795265\n",
      " -0.03684865  0.09776229  0.12399012 -0.18177772  0.00408614  0.03928367\n",
      "  0.09157463  0.04450165 -0.2534017  -0.04419255  0.01610222  0.01676961]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.6310446262359619, 'تاتو'),\n",
       " (0.5922053456306458, 'تتوی'),\n",
       " (0.5836590528488159, 'خالکوبی'),\n",
       " (0.5096367001533508, 'ﺗﺤﻤﻞ'),\n",
       " (0.47363975644111633, 'tattoo'),\n",
       " (0.4729554057121277, 'خالكوبي'),\n",
       " (0.46942010521888733, 'خالکوبي'),\n",
       " (0.46541884541511536, 'تاتوی'),\n",
       " (0.4653756320476532, 'خال\\u200cکوبی'),\n",
       " (0.4652386009693146, 'تَتو')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Unit test Fasttext word embedding model by similar word\n",
    "\n",
    "phrase = \"\\u062A\\u062A\\u0648\" #@param {type:\"string\"}\n",
    "print(\"dimension of \" + phrase + \" is \" +str(fasttext_model.get_dimension()))\n",
    "print(fasttext_model.get_word_vector(phrase).shape)\n",
    "print(fasttext_model[phrase]) # get the vector of the word \n",
    "\n",
    "# get similar word\n",
    "fasttext_model.get_nearest_neighbors(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Dataset Normalization and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZKbBDbX7Mza"
   },
   "source": [
    "in this step we going to collect a dataset that crawled by [@minasmz](https://github.com/minasmz) it's not good and I only used 450 pos and 450 neg reviews from it.anyway here we will download the dataset and split it to train and test ( I created Train and Test then I filled it with data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "nAQMVT05MMY4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************\n",
      "Posetive count 2905\n",
      "*Negetive count 438\n",
      "Natural  count 569\n",
      "Total dataset count 4009\n",
      "Total shuffle count 1438 \n",
      "\n",
      "Random Posetive Query:  \n",
      " ['بومینو همراه اول', 1.0]\n",
      "Random Negetive Query:  \n",
      " ['مشکل همراه اول', 2.0]\n",
      "Random unknown  Query:  \n",
      " ['شاهگوش', 3.0]\n"
     ]
    }
   ],
   "source": [
    "#@title Upload on google colab and prepare Dataset\n",
    "#!rm -rf /content/related-query-whit-lexion.csv\n",
    "#!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/related-query-whit-lexion.csv\n",
    "\n",
    "# load and read sentiment_tagged dataset.csv file in tne path ./content/ in google colab. \n",
    "# this dataset include three element: Comment,Score,Suggestion. Comment is feature and Suggestion is label.\n",
    "csv_dataset = pandas.read_csv(\"related-query-whit-lexion.csv\")\n",
    "\n",
    "def CleanPersianText(text):\n",
    "  _normalizer = hazm.Normalizer()\n",
    "  text = _normalizer.normalize(text)\n",
    "  return text\n",
    "\n",
    "# Cleansing the dataset and creating a new list with two elements: \"Comment\" and \"suggestion\"filde. (but without the third element: \"score\")\n",
    "# The new list is created by the zip command --> x= zip(csv_dataset['Comment'],csv_dataset['Suggestion'])\n",
    "# valu of suggestion is 1,2,3 or positive,negative,neutral\n",
    "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Query'],csv_dataset['Suggestion'])))\n",
    "\n",
    "# Separation of positive and negative suggestions\n",
    "positive=list(filter(lambda x: x[1] == 1,revlist))\n",
    "neutral=list(filter(lambda x: x[1] == 3,revlist))\n",
    "negative=list(filter(lambda x: x[1] == 2,revlist))\n",
    "\n",
    "# print number of element exist in positive, neutral, negative, revlist list \n",
    "print(\"*\" * 88)\n",
    "print(\"Posetive count {}\".format(len(positive)))\n",
    "print(\"*Negetive count {}\".format(len(negative)))\n",
    "print(\"Natural  count {}\".format(len(neutral)))\n",
    "print(\"Total dataset count {}\".format(len(revlist)))\n",
    "\n",
    "# mix positive and negative suggestions for 1438 element.\n",
    "# We chose 438 because the most negative comments were 438 = \"len(negative)\"\n",
    "revlist_shuffle = positive[:1000] + negative[:438]\n",
    "random.shuffle(revlist_shuffle)\n",
    "random.shuffle(revlist_shuffle)#double shuffle\n",
    "print(\"Total shuffle count {}\".format(len(revlist_shuffle)),\"\\n\")\n",
    "\n",
    "# print random element from positive, neutral, negative List\n",
    "print(\"Random Posetive Query: \",\"\\n\",positive[random.randrange(1,len(positive))])\n",
    "print(\"Random Negetive Query: \",\"\\n\",negative[random.randrange(1,len(negative))])\n",
    "print(\"Random unknown  Query: \",\"\\n\",neutral[random.randrange(1,len(neutral))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "wUJceKehjfJ3"
   },
   "outputs": [],
   "source": [
    "#@title create and Prepare Train & Test data_structure with zero value\n",
    "embedding_dim = 300 #@param {type:\"integer\"}\n",
    "max_vocab_token = 8 #@param {type:\"integer\"} #set 5 for related query in google trends\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "train_size = int(0.90*(len(revlist_shuffle)))\n",
    "test_size = int(0.10*(len(revlist_shuffle)))\n",
    "\n",
    "# x_train same as features and y_train same as the label. x_train same as input and y_train same as output.\n",
    "# The x_train data have 3 Dimention (874,10,300): (number_of_comment,number_of_words, dimension_of_fasttext)\n",
    "# The y_train data has 2 dimensions (874,2): (number of comments, suggestions)\n",
    "# The suggestions are 1 or 3. 1's are positive and 3's are negative suggestions.\n",
    "x_train = np.zeros((train_size, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
    "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
    "\n",
    "x_test = np.zeros((test_size, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
    "y_test = np.zeros((test_size, 2), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "jvcGBpjPwFL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_item is: 1437 \n",
      "\n",
      "(1294, 8, 300) (143, 8, 300) (1294, 2) (143, 2)\n"
     ]
    }
   ],
   "source": [
    "#@title Fill X_Train, X_Test, Y_Train, Y_Test by Dataset\n",
    "indexes = set(np.random.choice(len(revlist_shuffle), train_size + test_size, replace=False)) # for random selection\n",
    "print(\"data_item is: \" + str(len(indexes)),\"\\n\")\n",
    "\n",
    "for data_item, index in enumerate(indexes): # indexes include 920 items of comments\n",
    "  comment = hazm.word_tokenize(revlist_shuffle[index][0]) #[0] means the \"comment\" field in the .csv file\n",
    "  for vocabs in range(0,len(comment)):\n",
    "    if vocabs >= max_vocab_token: \n",
    "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
    "    if comment[vocabs] not in fasttext_model.words:\n",
    "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector in x_train is zero\n",
    "    if data_item < train_size:\n",
    "      x_train[data_item, vocabs, :] = fasttext_model.get_word_vector(comment[vocabs])\n",
    "    else:\n",
    "      x_test[data_item - train_size, vocabs, :] = fasttext_model.get_word_vector(comment[vocabs])\n",
    "\n",
    "  if data_item < train_size:\n",
    "    y_train[data_item, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 2 else [0.0, 1.0]\n",
    "  else:\n",
    "    y_test[data_item - train_size, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 2 else [0.0, 1.0]\n",
    "    \n",
    "print (x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Config & Compile & Fit the LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDunM15J7n8E"
   },
   "source": [
    "Now we will create our LSTM model then feed it our Train data\n",
    "\n",
    "This code will help you build a neural network model with LSTM, which is capable of predicting the level of delusion, i.e. the dangerousness of an opinion.\n",
    "First, we create the LSTM_model and add layers to it sequentially.\n",
    "First, a Conv1D layer is added to the model, which is used to convert each word into a suitable vector.\n",
    "In this model, two more Conv1D layers have been added to the model, which use 3x3 size filters.\n",
    "A MaxPooling1D layer with a window size of 3 is also added to the model because it helps reduce dimensionality (i.e. ease of processing).\n",
    "Then an LSTM layer with 512 neurons is added to the model, which uses long sentences for prediction.\n",
    "Then three perceptron layers with sigmoid activations are added to the model. The dimensions of these layers are 512, 512 and 512 respectively.\n",
    "To prevent overfitting, three Dropout layers with coefficients of 0.2 and 0.25 are used.\n",
    "Finally, a Dense layer is added to the model which is the number of desired decision output (in this case 2) and finally softmax is used as activation which returns the probabilities of the classes.\n",
    "The compile function is used to set the parameters of the model, where categorical_crossentropy is used as a loss function and is used for Adam optimization.\n",
    "At the end, by using model print, we get a summary of the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "J1z_mq913jTq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 8, 32)             153632    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 8, 48)             18480     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 8, 64)             24640     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 2, 64)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 1024)             2363392   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,611,282\n",
      "Trainable params: 3,611,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IT PHD 1403\\Sentiment-Analyzer\\Persian-Opinion-Mining-and-Sentiment-Analysis\\myenv\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#@title Building Layers of LSTM Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall #for Precision and Recall metric\n",
    "# Define precision as a metric object\n",
    "precision_metric = Precision()\n",
    "recall_metric = Recall()\n",
    "\n",
    "CNN_model = Sequential() \n",
    "\n",
    "# Firstly, we will add an embedding layer which will convert each word into vector \n",
    "# then set the hyperparameters of the layer\n",
    "# We use Conv1D because sentences have one dimension: Convolutional layer is 8x300 and filter(kernel_size)=32 3x3\n",
    "CNN_model.add(Conv1D(32, kernel_size=16, activation='elu', padding='same', input_shape=(max_vocab_token, embedding_dim)))\n",
    "\n",
    "CNN_model.add(Conv1D(48, kernel_size=12, activation='elu', padding='same'))\n",
    "CNN_model.add(Conv1D(64, kernel_size=8, activation='relu', padding='same'))\n",
    "CNN_model.add(MaxPooling1D(pool_size=4)) # Down sampling\n",
    "\n",
    "# Add LSTM layer whit 512 neuron & Dropout--> use for prevent of overfitting\n",
    "CNN_model.add(Bidirectional(LSTM(512, dropout=0.1, recurrent_dropout=0.2)))\n",
    "\n",
    "# \"Dense\" refers to a fully connected layer\n",
    "CNN_model.add(Dense(512, activation='sigmoid')) # sigmoid --> use for binary classification\n",
    "CNN_model.add(Dropout(0.5)) # Dropout--> use for prevent of overfitting\n",
    "CNN_model.add(Dense(512, activation='sigmoid'))\n",
    "CNN_model.add(Dropout(0.6))\n",
    "CNN_model.add(Dense(512, activation='sigmoid'))\n",
    "CNN_model.add(Dropout(0.7))\n",
    "\n",
    "# Dense 2 --> this layer is used to Decision between two classes.\n",
    "CNN_model.add(Dense(2, activation='softmax')) # softmax --> Returns the probability of a comment for each class.\n",
    "\n",
    "# categorical_crossentropy cost function is used for multi-category classification problems.\n",
    "# Adam's optimization algorithm parameters is used and lr=0.0001 determine the learning rate and decay=1e-6 determine step size reduction rate\n",
    "CNN_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001, decay=1e-6), metrics=['accuracy', 'AUC', precision_metric, recall_metric])\n",
    "\n",
    "# Show Dashboard\n",
    "#tensorboard = TensorBoard(log_dir='logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "print(CNN_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 7s 669ms/step - loss: 0.8010 - accuracy: 0.6136 - auc: 0.6554 - precision: 0.6136 - recall: 0.6136 - val_loss: 0.6054 - val_accuracy: 0.7063 - val_auc: 0.7063 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.7968 - accuracy: 0.5958 - auc: 0.6347 - precision: 0.5958 - recall: 0.5958 - val_loss: 0.6150 - val_accuracy: 0.7063 - val_auc: 0.7063 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 1s 255ms/step - loss: 0.7543 - accuracy: 0.6128 - auc: 0.6466 - precision: 0.6128 - recall: 0.6128 - val_loss: 0.6046 - val_accuracy: 0.7063 - val_auc: 0.7063 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.7442 - accuracy: 0.6229 - auc: 0.6539 - precision: 0.6229 - recall: 0.6229 - val_loss: 0.6140 - val_accuracy: 0.7063 - val_auc: 0.8223 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.7080 - accuracy: 0.6577 - auc: 0.6846 - precision: 0.6577 - recall: 0.6577 - val_loss: 0.5908 - val_accuracy: 0.7063 - val_auc: 0.8764 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.6781 - accuracy: 0.6422 - auc: 0.6719 - precision: 0.6422 - recall: 0.6422 - val_loss: 0.5541 - val_accuracy: 0.7063 - val_auc: 0.8839 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.6193 - accuracy: 0.6870 - auc: 0.7397 - precision: 0.6870 - recall: 0.6870 - val_loss: 0.4541 - val_accuracy: 0.7063 - val_auc: 0.9012 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.4825 - accuracy: 0.7666 - auc: 0.8528 - precision: 0.7666 - recall: 0.7666 - val_loss: 0.3199 - val_accuracy: 0.9091 - val_auc: 0.9693 - val_precision: 0.9091 - val_recall: 0.9091\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 1s 272ms/step - loss: 0.3468 - accuracy: 0.8779 - auc: 0.9378 - precision: 0.8779 - recall: 0.8779 - val_loss: 0.2373 - val_accuracy: 0.9231 - val_auc: 0.9679 - val_precision: 0.9231 - val_recall: 0.9231\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 1s 264ms/step - loss: 0.2471 - accuracy: 0.9382 - auc: 0.9532 - precision: 0.9382 - recall: 0.9382 - val_loss: 0.2074 - val_accuracy: 0.9441 - val_auc: 0.9615 - val_precision: 0.9441 - val_recall: 0.9441\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.1875 - accuracy: 0.9529 - auc: 0.9674 - precision: 0.9529 - recall: 0.9529 - val_loss: 0.2206 - val_accuracy: 0.9510 - val_auc: 0.9576 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.1934 - accuracy: 0.9552 - auc: 0.9637 - precision: 0.9552 - recall: 0.9552 - val_loss: 0.1730 - val_accuracy: 0.9650 - val_auc: 0.9619 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.1716 - accuracy: 0.9645 - auc: 0.9664 - precision: 0.9645 - recall: 0.9645 - val_loss: 0.2713 - val_accuracy: 0.9441 - val_auc: 0.9498 - val_precision: 0.9441 - val_recall: 0.9441\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.2070 - accuracy: 0.9575 - auc: 0.9571 - precision: 0.9575 - recall: 0.9575 - val_loss: 0.1747 - val_accuracy: 0.9650 - val_auc: 0.9704 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.1783 - accuracy: 0.9621 - auc: 0.9648 - precision: 0.9621 - recall: 0.9621 - val_loss: 0.1858 - val_accuracy: 0.9510 - val_auc: 0.9704 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.1567 - accuracy: 0.9668 - auc: 0.9695 - precision: 0.9668 - recall: 0.9668 - val_loss: 0.2476 - val_accuracy: 0.9441 - val_auc: 0.9425 - val_precision: 0.9441 - val_recall: 0.9441\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.1451 - accuracy: 0.9691 - auc: 0.9718 - precision: 0.9691 - recall: 0.9691 - val_loss: 0.1621 - val_accuracy: 0.9650 - val_auc: 0.9635 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.1488 - accuracy: 0.9706 - auc: 0.9680 - precision: 0.9706 - recall: 0.9706 - val_loss: 0.1713 - val_accuracy: 0.9650 - val_auc: 0.9592 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.1484 - accuracy: 0.9691 - auc: 0.9725 - precision: 0.9691 - recall: 0.9691 - val_loss: 0.1821 - val_accuracy: 0.9580 - val_auc: 0.9551 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.1450 - accuracy: 0.9683 - auc: 0.9728 - precision: 0.9683 - recall: 0.9683 - val_loss: 0.1841 - val_accuracy: 0.9510 - val_auc: 0.9562 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.1442 - accuracy: 0.9675 - auc: 0.9741 - precision: 0.9675 - recall: 0.9675 - val_loss: 0.1774 - val_accuracy: 0.9510 - val_auc: 0.9656 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.1253 - accuracy: 0.9730 - auc: 0.9791 - precision: 0.9730 - recall: 0.9730 - val_loss: 0.1918 - val_accuracy: 0.9510 - val_auc: 0.9562 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.1377 - accuracy: 0.9714 - auc: 0.9711 - precision: 0.9714 - recall: 0.9714 - val_loss: 0.1759 - val_accuracy: 0.9580 - val_auc: 0.9638 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.1500 - accuracy: 0.9699 - auc: 0.9688 - precision: 0.9699 - recall: 0.9699 - val_loss: 0.2040 - val_accuracy: 0.9510 - val_auc: 0.9565 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.1255 - accuracy: 0.9730 - auc: 0.9795 - precision: 0.9730 - recall: 0.9730 - val_loss: 0.2037 - val_accuracy: 0.9510 - val_auc: 0.9609 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.1281 - accuracy: 0.9722 - auc: 0.9775 - precision: 0.9722 - recall: 0.9722 - val_loss: 0.2100 - val_accuracy: 0.9510 - val_auc: 0.9602 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.1239 - accuracy: 0.9737 - auc: 0.9770 - precision: 0.9737 - recall: 0.9737 - val_loss: 0.1801 - val_accuracy: 0.9580 - val_auc: 0.9638 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.1255 - accuracy: 0.9745 - auc: 0.9743 - precision: 0.9745 - recall: 0.9745 - val_loss: 0.1737 - val_accuracy: 0.9580 - val_auc: 0.9680 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.1255 - accuracy: 0.9722 - auc: 0.9796 - precision: 0.9722 - recall: 0.9722 - val_loss: 0.1528 - val_accuracy: 0.9650 - val_auc: 0.9764 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.1153 - accuracy: 0.9768 - auc: 0.9788 - precision: 0.9768 - recall: 0.9768 - val_loss: 0.2027 - val_accuracy: 0.9510 - val_auc: 0.9682 - val_precision: 0.9510 - val_recall: 0.9510\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 0.1272 - accuracy: 0.9737 - auc: 0.9764 - precision: 0.9737 - recall: 0.9737 - val_loss: 0.2057 - val_accuracy: 0.9441 - val_auc: 0.9697 - val_precision: 0.9441 - val_recall: 0.9441\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.1231 - accuracy: 0.9730 - auc: 0.9788 - precision: 0.9730 - recall: 0.9730 - val_loss: 0.2497 - val_accuracy: 0.9301 - val_auc: 0.9706 - val_precision: 0.9301 - val_recall: 0.9301\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 1s 233ms/step - loss: 0.1183 - accuracy: 0.9753 - auc: 0.9797 - precision: 0.9753 - recall: 0.9753 - val_loss: 0.2077 - val_accuracy: 0.9441 - val_auc: 0.9736 - val_precision: 0.9441 - val_recall: 0.9441\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 0.1041 - accuracy: 0.9745 - auc: 0.9875 - precision: 0.9745 - recall: 0.9745 - val_loss: 0.1486 - val_accuracy: 0.9650 - val_auc: 0.9788 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.1097 - accuracy: 0.9784 - auc: 0.9826 - precision: 0.9784 - recall: 0.9784 - val_loss: 0.1284 - val_accuracy: 0.9790 - val_auc: 0.9777 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.1027 - accuracy: 0.9776 - auc: 0.9826 - precision: 0.9776 - recall: 0.9776 - val_loss: 0.1443 - val_accuracy: 0.9580 - val_auc: 0.9829 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0928 - accuracy: 0.9807 - auc: 0.9857 - precision: 0.9807 - recall: 0.9807 - val_loss: 0.1631 - val_accuracy: 0.9580 - val_auc: 0.9751 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.0975 - accuracy: 0.9768 - auc: 0.9854 - precision: 0.9768 - recall: 0.9768 - val_loss: 0.1613 - val_accuracy: 0.9650 - val_auc: 0.9725 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0970 - accuracy: 0.9784 - auc: 0.9865 - precision: 0.9784 - recall: 0.9784 - val_loss: 0.1541 - val_accuracy: 0.9580 - val_auc: 0.9822 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0927 - accuracy: 0.9799 - auc: 0.9850 - precision: 0.9799 - recall: 0.9799 - val_loss: 0.1452 - val_accuracy: 0.9580 - val_auc: 0.9857 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0784 - accuracy: 0.9807 - auc: 0.9902 - precision: 0.9807 - recall: 0.9807 - val_loss: 0.1421 - val_accuracy: 0.9580 - val_auc: 0.9853 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.0749 - accuracy: 0.9838 - auc: 0.9919 - precision: 0.9838 - recall: 0.9838 - val_loss: 0.1559 - val_accuracy: 0.9580 - val_auc: 0.9811 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0701 - accuracy: 0.9861 - auc: 0.9914 - precision: 0.9861 - recall: 0.9861 - val_loss: 0.1218 - val_accuracy: 0.9650 - val_auc: 0.9872 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.0656 - accuracy: 0.9869 - auc: 0.9922 - precision: 0.9869 - recall: 0.9869 - val_loss: 0.1019 - val_accuracy: 0.9790 - val_auc: 0.9883 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0637 - accuracy: 0.9845 - auc: 0.9925 - precision: 0.9845 - recall: 0.9845 - val_loss: 0.1070 - val_accuracy: 0.9720 - val_auc: 0.9883 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 1s 266ms/step - loss: 0.0598 - accuracy: 0.9869 - auc: 0.9932 - precision: 0.9869 - recall: 0.9869 - val_loss: 0.1196 - val_accuracy: 0.9720 - val_auc: 0.9874 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0561 - accuracy: 0.9892 - auc: 0.9940 - precision: 0.9892 - recall: 0.9892 - val_loss: 0.1216 - val_accuracy: 0.9720 - val_auc: 0.9845 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0563 - accuracy: 0.9861 - auc: 0.9940 - precision: 0.9861 - recall: 0.9861 - val_loss: 0.1159 - val_accuracy: 0.9720 - val_auc: 0.9888 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0508 - accuracy: 0.9900 - auc: 0.9939 - precision: 0.9900 - recall: 0.9900 - val_loss: 0.1178 - val_accuracy: 0.9720 - val_auc: 0.9897 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.0474 - accuracy: 0.9907 - auc: 0.9941 - precision: 0.9907 - recall: 0.9907 - val_loss: 0.1218 - val_accuracy: 0.9720 - val_auc: 0.9838 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0472 - accuracy: 0.9907 - auc: 0.9953 - precision: 0.9907 - recall: 0.9907 - val_loss: 0.1213 - val_accuracy: 0.9720 - val_auc: 0.9842 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0487 - accuracy: 0.9907 - auc: 0.9929 - precision: 0.9907 - recall: 0.9907 - val_loss: 0.1262 - val_accuracy: 0.9720 - val_auc: 0.9847 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 1s 277ms/step - loss: 0.0429 - accuracy: 0.9923 - auc: 0.9949 - precision: 0.9923 - recall: 0.9923 - val_loss: 0.1348 - val_accuracy: 0.9720 - val_auc: 0.9847 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 1s 257ms/step - loss: 0.0423 - accuracy: 0.9915 - auc: 0.9959 - precision: 0.9915 - recall: 0.9915 - val_loss: 0.1315 - val_accuracy: 0.9720 - val_auc: 0.9850 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.0441 - accuracy: 0.9923 - auc: 0.9943 - precision: 0.9923 - recall: 0.9923 - val_loss: 0.1366 - val_accuracy: 0.9720 - val_auc: 0.9851 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0382 - accuracy: 0.9930 - auc: 0.9955 - precision: 0.9930 - recall: 0.9930 - val_loss: 0.1376 - val_accuracy: 0.9720 - val_auc: 0.9851 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 1s 274ms/step - loss: 0.0404 - accuracy: 0.9915 - auc: 0.9951 - precision: 0.9915 - recall: 0.9915 - val_loss: 0.1422 - val_accuracy: 0.9720 - val_auc: 0.9847 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.0365 - accuracy: 0.9938 - auc: 0.9962 - precision: 0.9938 - recall: 0.9938 - val_loss: 0.1494 - val_accuracy: 0.9720 - val_auc: 0.9844 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.0401 - accuracy: 0.9930 - auc: 0.9940 - precision: 0.9930 - recall: 0.9930 - val_loss: 0.1481 - val_accuracy: 0.9720 - val_auc: 0.9844 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 1s 286ms/step - loss: 0.0345 - accuracy: 0.9930 - auc: 0.9952 - precision: 0.9930 - recall: 0.9930 - val_loss: 0.1399 - val_accuracy: 0.9720 - val_auc: 0.9847 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 1s 268ms/step - loss: 0.0356 - accuracy: 0.9938 - auc: 0.9948 - precision: 0.9938 - recall: 0.9938 - val_loss: 0.1546 - val_accuracy: 0.9720 - val_auc: 0.9842 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.0329 - accuracy: 0.9954 - auc: 0.9967 - precision: 0.9954 - recall: 0.9954 - val_loss: 0.1465 - val_accuracy: 0.9720 - val_auc: 0.9848 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.0325 - accuracy: 0.9946 - auc: 0.9966 - precision: 0.9946 - recall: 0.9946 - val_loss: 0.1458 - val_accuracy: 0.9720 - val_auc: 0.9851 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.0300 - accuracy: 0.9946 - auc: 0.9963 - precision: 0.9946 - recall: 0.9946 - val_loss: 0.1525 - val_accuracy: 0.9720 - val_auc: 0.9849 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 1s 273ms/step - loss: 0.0315 - accuracy: 0.9946 - auc: 0.9951 - precision: 0.9946 - recall: 0.9946 - val_loss: 0.1515 - val_accuracy: 0.9720 - val_auc: 0.9782 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0268 - accuracy: 0.9954 - auc: 0.9966 - precision: 0.9954 - recall: 0.9954 - val_loss: 0.1601 - val_accuracy: 0.9650 - val_auc: 0.9781 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 1s 255ms/step - loss: 0.0263 - accuracy: 0.9954 - auc: 0.9970 - precision: 0.9954 - recall: 0.9954 - val_loss: 0.1322 - val_accuracy: 0.9720 - val_auc: 0.9850 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 0.0281 - accuracy: 0.9954 - auc: 0.9960 - precision: 0.9954 - recall: 0.9954 - val_loss: 0.0980 - val_accuracy: 0.9720 - val_auc: 0.9922 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0349 - accuracy: 0.9938 - auc: 0.9948 - precision: 0.9938 - recall: 0.9938 - val_loss: 0.1588 - val_accuracy: 0.9650 - val_auc: 0.9849 - val_precision: 0.9650 - val_recall: 0.9650\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0396 - accuracy: 0.9923 - auc: 0.9951 - precision: 0.9923 - recall: 0.9923 - val_loss: 0.0947 - val_accuracy: 0.9790 - val_auc: 0.9854 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0350 - accuracy: 0.9946 - auc: 0.9947 - precision: 0.9946 - recall: 0.9946 - val_loss: 0.1464 - val_accuracy: 0.9720 - val_auc: 0.9849 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 0.0347 - accuracy: 0.9946 - auc: 0.9955 - precision: 0.9946 - recall: 0.9946 - val_loss: 0.2001 - val_accuracy: 0.9580 - val_auc: 0.9768 - val_precision: 0.9580 - val_recall: 0.9580\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 1s 304ms/step - loss: 0.0279 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1685 - val_accuracy: 0.9720 - val_auc: 0.9774 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 1s 324ms/step - loss: 0.0254 - accuracy: 0.9961 - auc: 0.9979 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1458 - val_accuracy: 0.9720 - val_auc: 0.9844 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 1s 355ms/step - loss: 0.0271 - accuracy: 0.9961 - auc: 0.9959 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1237 - val_accuracy: 0.9720 - val_auc: 0.9846 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0254 - accuracy: 0.9961 - auc: 0.9976 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.0999 - val_accuracy: 0.9860 - val_auc: 0.9855 - val_precision: 0.9860 - val_recall: 0.9860\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.0264 - accuracy: 0.9961 - auc: 0.9970 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.0990 - val_accuracy: 0.9860 - val_auc: 0.9855 - val_precision: 0.9860 - val_recall: 0.9860\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.0277 - accuracy: 0.9961 - auc: 0.9955 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1488 - val_accuracy: 0.9720 - val_auc: 0.9850 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.0230 - accuracy: 0.9961 - auc: 0.9984 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1710 - val_accuracy: 0.9720 - val_auc: 0.9782 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 0.0266 - accuracy: 0.9961 - auc: 0.9965 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1636 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 0.0277 - accuracy: 0.9961 - auc: 0.9951 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1606 - val_accuracy: 0.9720 - val_auc: 0.9785 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.0275 - accuracy: 0.9961 - auc: 0.9952 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1703 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.0241 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1776 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.0261 - accuracy: 0.9961 - auc: 0.9971 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1808 - val_accuracy: 0.9720 - val_auc: 0.9783 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 1s 350ms/step - loss: 0.0289 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1823 - val_accuracy: 0.9720 - val_auc: 0.9783 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0253 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1822 - val_accuracy: 0.9720 - val_auc: 0.9783 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 1s 392ms/step - loss: 0.0258 - accuracy: 0.9961 - auc: 0.9965 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1804 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 1s 350ms/step - loss: 0.0234 - accuracy: 0.9961 - auc: 0.9988 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1782 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.0242 - accuracy: 0.9961 - auc: 0.9976 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1763 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0262 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1745 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 1s 399ms/step - loss: 0.0250 - accuracy: 0.9961 - auc: 0.9976 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1727 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 0.0260 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1712 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 0.0248 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1703 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.0254 - accuracy: 0.9961 - auc: 0.9960 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1697 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 1s 323ms/step - loss: 0.0277 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1689 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9960 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1683 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 1s 320ms/step - loss: 0.0251 - accuracy: 0.9961 - auc: 0.9973 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1678 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.0263 - accuracy: 0.9961 - auc: 0.9956 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1674 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.0248 - accuracy: 0.9961 - auc: 0.9969 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1671 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0238 - accuracy: 0.9961 - auc: 0.9979 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1668 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1665 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.0252 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1665 - val_accuracy: 0.9790 - val_auc: 0.9784 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1665 - val_accuracy: 0.9790 - val_auc: 0.9784 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.0244 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1668 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.0275 - accuracy: 0.9961 - auc: 0.9960 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1674 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.0264 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1679 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9958 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1683 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0263 - accuracy: 0.9961 - auc: 0.9965 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1689 - val_accuracy: 0.9720 - val_auc: 0.9785 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9965 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1694 - val_accuracy: 0.9720 - val_auc: 0.9785 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 0.0284 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1699 - val_accuracy: 0.9720 - val_auc: 0.9785 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 1s 299ms/step - loss: 0.0276 - accuracy: 0.9961 - auc: 0.9963 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1701 - val_accuracy: 0.9720 - val_auc: 0.9785 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 0.0257 - accuracy: 0.9961 - auc: 0.9967 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1702 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.0249 - accuracy: 0.9961 - auc: 0.9968 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1704 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0264 - accuracy: 0.9961 - auc: 0.9963 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1704 - val_accuracy: 0.9720 - val_auc: 0.9784 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0253 - accuracy: 0.9961 - auc: 0.9973 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1703 - val_accuracy: 0.9720 - val_auc: 0.9785 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 0.0254 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1702 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0281 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1702 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.0277 - accuracy: 0.9961 - auc: 0.9951 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1701 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.0254 - accuracy: 0.9961 - auc: 0.9971 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1700 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0245 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1699 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1700 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.0275 - accuracy: 0.9961 - auc: 0.9956 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1698 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 1s 279ms/step - loss: 0.0248 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1698 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 1s 285ms/step - loss: 0.0253 - accuracy: 0.9961 - auc: 0.9971 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1700 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0287 - accuracy: 0.9961 - auc: 0.9950 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1702 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0246 - accuracy: 0.9961 - auc: 0.9969 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1702 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 1s 303ms/step - loss: 0.0259 - accuracy: 0.9961 - auc: 0.9959 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1703 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 1s 301ms/step - loss: 0.0266 - accuracy: 0.9961 - auc: 0.9963 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1703 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 1s 307ms/step - loss: 0.0265 - accuracy: 0.9961 - auc: 0.9962 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1702 - val_accuracy: 0.9790 - val_auc: 0.9784 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.0275 - accuracy: 0.9961 - auc: 0.9961 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1702 - val_accuracy: 0.9790 - val_auc: 0.9784 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 1s 317ms/step - loss: 0.0250 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1703 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 1s 311ms/step - loss: 0.0252 - accuracy: 0.9961 - auc: 0.9965 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1706 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.0261 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1707 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.0272 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1708 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 1s 315ms/step - loss: 0.0249 - accuracy: 0.9961 - auc: 0.9967 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1709 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.0272 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1709 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.0242 - accuracy: 0.9961 - auc: 0.9971 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1710 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.0231 - accuracy: 0.9961 - auc: 0.9973 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1712 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.0252 - accuracy: 0.9961 - auc: 0.9967 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1715 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 1s 340ms/step - loss: 0.0251 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1717 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0253 - accuracy: 0.9961 - auc: 0.9971 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1719 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.0241 - accuracy: 0.9961 - auc: 0.9974 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1722 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9951 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1724 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0218 - accuracy: 0.9961 - auc: 0.9986 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1728 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0246 - accuracy: 0.9961 - auc: 0.9967 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1731 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 0.0264 - accuracy: 0.9961 - auc: 0.9958 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1734 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 1s 323ms/step - loss: 0.0237 - accuracy: 0.9961 - auc: 0.9975 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1737 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.0231 - accuracy: 0.9961 - auc: 0.9986 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1739 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 1s 392ms/step - loss: 0.0270 - accuracy: 0.9961 - auc: 0.9950 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1740 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.0255 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1741 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 1s 323ms/step - loss: 0.0246 - accuracy: 0.9961 - auc: 0.9977 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1742 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.0279 - accuracy: 0.9961 - auc: 0.9949 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1743 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.0269 - accuracy: 0.9961 - auc: 0.9956 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1743 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 1s 325ms/step - loss: 0.0270 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1745 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 1s 387ms/step - loss: 0.0267 - accuracy: 0.9961 - auc: 0.9951 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1745 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 1s 395ms/step - loss: 0.0270 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1747 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 1s 385ms/step - loss: 0.0287 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1747 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 1s 415ms/step - loss: 0.0243 - accuracy: 0.9961 - auc: 0.9973 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1748 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 1s 355ms/step - loss: 0.0245 - accuracy: 0.9961 - auc: 0.9973 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1751 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.0230 - accuracy: 0.9961 - auc: 0.9983 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1754 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 1s 380ms/step - loss: 0.0259 - accuracy: 0.9961 - auc: 0.9973 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1756 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 1s 346ms/step - loss: 0.0258 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1759 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 0.0271 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1762 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.0259 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1764 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.0279 - accuracy: 0.9961 - auc: 0.9951 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1763 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9960 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1764 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.0244 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1764 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0251 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1765 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.0241 - accuracy: 0.9961 - auc: 0.9965 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1766 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 0.0254 - accuracy: 0.9961 - auc: 0.9963 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1768 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 0.0256 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1769 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 1s 282ms/step - loss: 0.0264 - accuracy: 0.9961 - auc: 0.9963 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1770 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 0.0243 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1772 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.0246 - accuracy: 0.9961 - auc: 0.9977 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1776 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0262 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1778 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0236 - accuracy: 0.9961 - auc: 0.9975 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1779 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 1s 273ms/step - loss: 0.0255 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1781 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0260 - accuracy: 0.9961 - auc: 0.9965 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1782 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0264 - accuracy: 0.9961 - auc: 0.9958 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1782 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.0262 - accuracy: 0.9961 - auc: 0.9955 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1782 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0265 - accuracy: 0.9961 - auc: 0.9963 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1782 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0258 - accuracy: 0.9961 - auc: 0.9959 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1782 - val_accuracy: 0.9790 - val_auc: 0.9786 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.0249 - accuracy: 0.9961 - auc: 0.9972 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1783 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 1s 291ms/step - loss: 0.0249 - accuracy: 0.9961 - auc: 0.9971 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1784 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 1s 301ms/step - loss: 0.0227 - accuracy: 0.9961 - auc: 0.9980 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1785 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 0.0263 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1786 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.0272 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1786 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.0227 - accuracy: 0.9961 - auc: 0.9982 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1788 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 1s 304ms/step - loss: 0.0251 - accuracy: 0.9961 - auc: 0.9963 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1789 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 1s 385ms/step - loss: 0.0231 - accuracy: 0.9961 - auc: 0.9979 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1791 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 0.0253 - accuracy: 0.9961 - auc: 0.9959 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1793 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0268 - accuracy: 0.9961 - auc: 0.9964 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1795 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.0234 - accuracy: 0.9961 - auc: 0.9976 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1796 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0232 - accuracy: 0.9961 - auc: 0.9976 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1798 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0257 - accuracy: 0.9961 - auc: 0.9967 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1798 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.0260 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1800 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 0.0267 - accuracy: 0.9961 - auc: 0.9957 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1801 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 0.0285 - accuracy: 0.9961 - auc: 0.9950 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1803 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.0248 - accuracy: 0.9961 - auc: 0.9973 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1805 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0264 - accuracy: 0.9961 - auc: 0.9966 - precision: 0.9961 - recall: 0.9961 - val_loss: 0.1806 - val_accuracy: 0.9790 - val_auc: 0.9787 - val_precision: 0.9790 - val_recall: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x202c240ad70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Set batchSize and epochs and start learning\n",
    "\n",
    "# batch_size: is the number of data to be selected in each step\n",
    "batch_size = 500 #@param {type:\"integer\"}\n",
    "no_epochs = 200 #@param {type:\"integer\"}\n",
    "\n",
    "#start learning\n",
    "CNN_model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Evaluate and Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this step we evaluate LSTM model loss and accuracy metric\n",
    "loss: 0.5849 - accuracy: 0.8333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TKsHaoO7HpW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy', 'auc', 'precision', 'recall']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ارزیابی کردن  مدل جدید با دیتاست تست قدیمی بدون آنکه لکسیون به دیتاست اضافه شده باشد در این محل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpocYJkB7KMs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 20ms/step - loss: 0.1806 - accuracy: 0.9790 - auc: 0.9787 - precision: 0.9790 - recall: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18056553602218628,\n",
       " 0.9790209531784058,\n",
       " 0.9787275195121765,\n",
       " 0.9790209531784058,\n",
       " 0.9790209531784058]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model evaluate\n",
    "CNN_model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5xPkhZX7Qmq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learned-query-sentiment-fasttext.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learned-query-sentiment-fasttext.model\\assets\n",
      "'zip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m CNN_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearned-query-sentiment-fasttext.model\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzip -r learned-query-sentiment-fasttext.model.zip learned-query-sentiment-fasttext.model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      7\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearned-query-sentiment-fasttext.model.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Save, zip and download the model for future use\n",
    "CNN_model.save('learned-query-sentiment-fasttext.model') \n",
    "\n",
    "!zip -r learned-query-sentiment-fasttext.model.zip learned-query-sentiment-fasttext.model\n",
    "\n",
    "from google.colab import files\n",
    "files.download('learned-query-sentiment-fasttext.model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJxZj2vr7uMO"
   },
   "source": [
    "### There are three ways to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "xXt5rQ0qmyax"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14616\\2897243695.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 558ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>99 % 😍</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>0 % 🤕</h4></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title using model 1\n",
    "\n",
    "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "if not user_text==\"\":\n",
    "  normal_text = _normalizer.normalize(user_text)\n",
    "  tokenized_text = hazm.word_tokenize(normal_text)\n",
    "  \n",
    "  # create and Prepare three dimension tensor (1,20,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "\n",
    "  for vocabs in range(0,len(tokenized_text)):\n",
    "    if vocabs >= max_vocab_token:\n",
    "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
    "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "    \n",
    "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "\n",
    "  # print(vector_text.shape)\n",
    "  # print(vector_text)\n",
    "  result = CNN_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
    "  pos_percent = str(int(result[0][1]*100))+\" % 😍\"\n",
    "  neg_percent = str(int(result[0][0]*100))+\" % 🤕\"\n",
    "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "else:\n",
    "  print(\"Please enter your text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "_kLhLv1h7UD_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14616\\1865725138.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>99 % 😍</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>0 % 🤕</h4></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title using model 2\n",
    "\n",
    "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "if not user_text==\"\":\n",
    "  normal_text = _normalizer.normalize(user_text)\n",
    "  tokenized_text = hazm.word_tokenize(normal_text)\n",
    "  \n",
    "  # create and Prepare three dimension tensor (1,20,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "  for vocabs in range(0,len(tokenized_text)):\n",
    "    if vocabs >= max_vocab_token:\n",
    "      break # If the comment is more than twenty words, only the first twenty words will be considered\n",
    "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "    \n",
    "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "\n",
    "  # print(vector_text.shape)\n",
    "  # print(vector_text)\n",
    "  result = CNN_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
    "  pos_percent = str(int(result[0][1]*100))+\" % 😍\"\n",
    "  neg_percent = str(int(result[0][0]*100))+\" % 🤕\"\n",
    "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "else:\n",
    "  print(\"Please enter your text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14616\\1199773519.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>0 % </h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>99 % </h4></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title using model 3\n",
    "\n",
    "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u062C\\u0627\\u0644\\u0628\\u0647 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0627\\u0635\\u0644\\u0627 \\u0647\\u0645\\u0647 \\u0686\\u06CC \\u062A\\u0645\\u0627\\u0645\\u0647 \\u0645\\u0646 \\u06A9\\u0647 \\u067E\\u0633\\u0646\\u062F\\u06CC\\u062F\\u0645 \\u0627\\u06CC\\u0646 \\u0645\\u0648\\u0628\\u0627\\u06CC\\u0644 \\u0632\\u06CC\\u0628\\u0627 \\u0631\\u0648\" #@param {type:\"string\"}\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "if not user_text==\"\":\n",
    "  normal_text = _normalizer.normalize(user_text)\n",
    "  tokenized_text = hazm.word_tokenize(normal_text)\n",
    "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "  for vocabs in range(0,len(tokenized_text)):\n",
    "    if vocabs >= max_vocab_token:\n",
    "      break\n",
    "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "      continue\n",
    "    \n",
    "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "  # print(x_text_for_test_words.shape)\n",
    "  # print(text_for_test_words)\n",
    "  result = CNN_model.predict(vector_text)\n",
    "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
    "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
    "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260205.svg'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://image.flaticon.com/icons/svg/260/260206.svg'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "else:\n",
    "  print(\"Please enter your text\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
