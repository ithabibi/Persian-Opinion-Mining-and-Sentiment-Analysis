{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/blob/main/use-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMNPBU2OwSBw"
   },
   "source": [
    "# Persian Sentiment Analysis With Fasttext language Model and LSTM neural network\n",
    "### Persian sentiment analysis step by step guide\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "so there are 3 steps we going through with each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Load fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pybind11==2.11.1\n",
    "!pip install fasttext==0.9.2 \n",
    "\n",
    "#!pip install keras==2.14.0\n",
    "!pip install tensorflow==2.12.0 #For Deep Learning\n",
    "!pip install keras==2.12.0 #A wrapper for TensorFlow for simplicity\n",
    "\n",
    "!pip install hazm==0.7.0\n",
    "!pip install pandas==1.5.3\n",
    "!pip install numpy==1.23\n",
    "\n",
    "import pandas\n",
    "import random\n",
    "import numpy as np\n",
    "import hazm\n",
    "import keras.backend as K\n",
    "import fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and unzip ELM\n",
    "!rm -rf /content/cc.fa.300.bin.gz\n",
    "!rm -rf /content/cc.fa.300.bin\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
    "!gunzip /content/cc.fa.300.bin.gz\n",
    "\n",
    "%time\n",
    "fasttext_model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Load LSTM learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and unzip learned model\n",
    "!rm -rf /content/learned-query-sentiment-fasttext.model.zip\n",
    "!rm -rf /content/learned-query-sentiment-fasttext.model\n",
    "\n",
    "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/learned-query-sentiment-fasttext.model.zip\n",
    "!unzip /content/learned-query-sentiment-fasttext.model.zip\n",
    "embedding_dim = 300 #@param {type:\"integer\"} #The number 300, is the dimensions of the model\n",
    "max_vocab_token = 8 #@param {type:\"integer\"}\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "#del model  # deletes the existing model\n",
    "\n",
    "LSTM_model = load_model('/content/learned-query-sentiment-fasttext.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 enter persian text and booooom!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title using model\n",
    "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "if not user_text==\"\":\n",
    "  normal_text = _normalizer.normalize(user_text)\n",
    "  tokenized_text = hazm.word_tokenize(normal_text)\n",
    "  \n",
    "  # create and Prepare three dimension tensor (1,8,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "  for vocabs in range(0,len(tokenized_text)):\n",
    "    if vocabs >= max_vocab_token:\n",
    "      break # If the comment is more than 8 words, only the first 8 words will be considered\n",
    "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "    \n",
    "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "\n",
    "  # print(vector_text.shape)\n",
    "  # print(vector_text)\n",
    "  result = LSTM_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
    "  pos_percent = str(int(result[0][1]*100))+\" % üòç\"\n",
    "  neg_percent = str(int(result[0][0]*100))+\" % ü§ï\"\n",
    "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "else:\n",
    "  print(\"Please enter your text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1 enter batch persian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title laod test Dataset\n",
    "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/related-query-whit-lexion.csv\n",
    "\n",
    "# load and read sentiment_tagged dataset.csv file in tÿße path ./content/ in google colab. \n",
    "# this dataset include three element: Query,Score,Suggestion. Query is feature and Suggestion is label.\n",
    "csv_dataset = pandas.read_csv(\"/content/related-query-whit-lexion.csv\")\n",
    "\n",
    "def CleanPersianText(text):\n",
    "  _normalizer = hazm.Normalizer()\n",
    "  text = _normalizer.normalize(text)\n",
    "  return text\n",
    "\n",
    "# Cleansing the dataset and creating a new list with two elements: \"Query\" and \"suggestion\"filde. (but without the third element: \"Score\")\n",
    "# The new list is created by the zip Query --> x= zip(csv_dataset['Query'],csv_dataset['Suggestion'])\n",
    "# valu of suggestion is 1,2,3 or positive,negative,neutral\n",
    "revlist = list(map(lambda x: [CleanPersianText(x[0]),\"1\",\"2\"],zip(csv_dataset['Query'],csv_dataset['Suggestion'],csv_dataset['Score'])))\n",
    "\n",
    "# print number of element exist in positive, neutral, negative, revlist list \n",
    "print(\"*\" * 88)\n",
    "print(\"Total dataset count {}\".format(len(revlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Result\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "for item in range(0,100): #len(revlist)\n",
    "  user_text = revlist[item][0]\n",
    "  if not user_text==\"\":\n",
    "    normal_text = _normalizer.normalize(user_text)\n",
    "    tokenized_text = hazm.word_tokenize(normal_text)\n",
    "    # create and Prepare three dimension tensor (1,8,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "    vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "    for vocabs in range(0,len(tokenized_text)):\n",
    "      if vocabs >= max_vocab_token:\n",
    "        break # If the comment is more than 8 words, only the first 8 words will be considered\n",
    "      if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "        continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "      vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "    \n",
    "    result = LSTM_model.predict(vector_text, verbose='0',workers=10,use_multiprocessing=True,max_queue_size=100) # the result has two element: [0][1] and [0][0]\n",
    "    pos_percent = str(int(result[0][1]*100))\n",
    "    neg_percent = str(int(result[0][0]*100))\n",
    "    #display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "    print(str(item) + \": \" + pos_percent +\"%üòç\" +\" \" + neg_percent +\"%ü§ï\" + \" \" + revlist[item][0] , \"\\n\")\n",
    "  else:\n",
    "    print(\"Please enter your text\")\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⁄©ÿØ ÿ≤€åÿ± ÿ±ÿß ÿ®Ÿá ŸÜÿ≠Ÿà€å ÿ™ÿ∫€å€åÿ± ÿ®ÿØŸá ⁄©Ÿá ÿÆÿ±Ÿàÿ¨€å result ÿ®Ÿá revlist ÿßÿ∂ÿßŸÅŸá ÿ¥ŸàÿØ Ÿà ŸÜŸáÿß€åÿ™ÿß ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© ŸÅÿß€åŸÑ csv ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ŸàÿØ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hazm\n",
    "import numpy as np\n",
    "\n",
    "# ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿØÿßÿØŸá‚ÄåŸáÿß\n",
    "csv_dataset = pd.read_csv(\"/content/merged_all_operator-data.from2009to2023B.csv\")\n",
    "\n",
    "def CleanPersianText(text):\n",
    "    _normalizer = hazm.Normalizer()\n",
    "    text = _normalizer.normalize(text)\n",
    "    return text\n",
    "\n",
    "# ÿß€åÿ¨ÿßÿØ revlist\n",
    "revlist = list(map(lambda x: [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\", CleanPersianText(x[6])], \n",
    "                   zip(csv_dataset['index'], csv_dataset['top25'], csv_dataset['value'], \n",
    "                       csv_dataset['date'], csv_dataset['keyword'], csv_dataset['get_type'], \n",
    "                       csv_dataset['Query'])))\n",
    "\n",
    "# ŸÜŸÖÿß€åÿ¥ ÿ™ÿπÿØÿßÿØ ⁄©ŸÑ ÿØÿßÿØŸá‚ÄåŸáÿß\n",
    "print(\"*\" * 88)\n",
    "print(\"Total dataset count {}\".format(len(revlist)))\n",
    "\n",
    "# ÿ¢ŸÖÿßÿØŸá‚Äåÿ≥ÿßÿ≤€å ÿ®ÿ±ÿß€å Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å\n",
    "_normalizer = hazm.Normalizer()\n",
    "results = []  # ŸÑ€åÿ≥ÿ™ ÿ®ÿ±ÿß€å ÿ∞ÿÆ€åÿ±Ÿá ŸÜÿ™ÿß€åÿ¨\n",
    "\n",
    "for item in range(0, 50):  # len(revlist)\n",
    "    user_text = revlist[item][6]\n",
    "    if user_text != \"\":\n",
    "        normal_text = _normalizer.normalize(user_text)\n",
    "        tokenized_text = hazm.word_tokenize(normal_text)\n",
    "\n",
    "        # ÿß€åÿ¨ÿßÿØ ÿ™ŸÜÿ≥Ÿàÿ± ÿ≥Ÿá ÿ®ÿπÿØ€å (1,8,300) ÿ®ÿß ŸÖŸÇÿØÿßÿ± ÿµŸÅÿ±\n",
    "        vector_text = np.zeros((1, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
    "\n",
    "        for vocabs in range(0, len(tokenized_text)):\n",
    "            if vocabs >= max_vocab_token:\n",
    "                break  # ÿß⁄Øÿ± ⁄©ÿßŸÖŸÜÿ™ ÿ®€åÿ¥ÿ™ÿ± ÿßÿ≤ 8 ⁄©ŸÑŸÖŸá ÿ®ÿßÿ¥ÿØÿå ŸÅŸÇÿ∑ 8 ⁄©ŸÑŸÖŸá ÿßŸàŸÑ ÿØÿ± ŸÜÿ∏ÿ± ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ\n",
    "            if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "                continue  # ÿß⁄Øÿ± Ÿàÿß⁄òŸá ÿØÿ± fasttext Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ÿØÿå Ÿáÿ± 300 ÿπŸÜÿµÿ± ÿ¢ŸÜ Ÿà⁄©ÿ™Ÿàÿ± ÿµŸÅÿ± ÿ®ÿßŸÇ€å ŸÖ€å‚ÄåŸÖÿßŸÜÿØ\n",
    "            vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "\n",
    "        result = LSTM_model.predict(vector_text, verbose='0', workers=10, use_multiprocessing=True, max_queue_size=100)\n",
    "        pos_percent = str(int(result[0][1] * 100))\n",
    "        neg_percent = str(int(result[0][0] * 100))\n",
    "\n",
    "        # ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ ŸÜÿ™ÿß€åÿ¨ ÿ®Ÿá revlist\n",
    "        revlist[item].append(pos_percent)\n",
    "        revlist[item].append(neg_percent)\n",
    "\n",
    "# ÿ™ÿ®ÿØ€åŸÑ revlist ÿ®Ÿá DataFrame\n",
    "columns = ['Index', 'Top25', 'Value', 'Date', 'Keyword', 'Get Type', 'Comment', 'Positive Percentage', 'Negative Percentage']\n",
    "results_df = pd.DataFrame(revlist, columns=columns)\n",
    "\n",
    "# ÿ∞ÿÆ€åÿ±Ÿá DataFrame ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© ŸÅÿß€åŸÑ CSV\n",
    "results_df.to_csv('results_with_predictions.csv', index=False)\n",
    "\n",
    "print(\"Results saved to results_with_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⁄©ÿØ ÿ≤€åÿ± ÿ±ÿß ÿ®Ÿá ŸÜÿ≠Ÿà€å ÿ™ÿ∫€å€åÿ± ÿ®ÿØŸá ⁄©Ÿá ÿÆÿ±Ÿàÿ¨€å result ÿ®Ÿá revlist ÿßÿ∂ÿßŸÅŸá ÿ¥ŸàÿØ ÿ®Ÿá ÿµŸàÿ±ÿ™€å ⁄©Ÿá €å⁄© ŸÅ€åŸÑÿØ ÿßÿ≠ÿ≥ÿßÿ≥ ÿßÿ∂ÿßŸÅŸá ÿ¥ŸàÿØ Ÿà ÿß⁄Øÿ± ŸÖÿ´ÿ®ÿ™ ÿ®ÿßÿ¥ÿØ ÿØÿ±ŸàŸÜ ÿ¢ŸÜ ÿ®ÿß ÿπÿ®ÿßÿ±ÿ™ positive Ÿà ÿß⁄Øÿ± ŸÖŸÜŸÅ€å ÿßÿ≥ÿ™ ÿ®ÿß ÿπÿ®ÿßÿ±ÿ™ negative Ÿæÿ± ÿ¥ŸàÿØ Ÿà ŸÜŸáÿß€åÿ™ÿß ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© ŸÅÿß€åŸÑ csv ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ŸàÿØ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hazm\n",
    "import numpy as np\n",
    "\n",
    "# ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿØÿßÿØŸá‚ÄåŸáÿß\n",
    "csv_dataset = pd.read_csv(\"/content/merged_all_operator-data.from2009to2023B.csv\")\n",
    "\n",
    "def CleanPersianText(text):\n",
    "    _normalizer = hazm.Normalizer()\n",
    "    text = _normalizer.normalize(text)\n",
    "    return text\n",
    "\n",
    "# ÿß€åÿ¨ÿßÿØ revlist\n",
    "revlist = list(map(lambda x: [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", CleanPersianText(x[6])], \n",
    "                   zip(csv_dataset['index'], csv_dataset['top25'], csv_dataset['value'], \n",
    "                       csv_dataset['date'], csv_dataset['keyword'], csv_dataset['get_type'], \n",
    "                       csv_dataset['Query'])))\n",
    "\n",
    "# ŸÜŸÖÿß€åÿ¥ ÿ™ÿπÿØÿßÿØ ⁄©ŸÑ ÿØÿßÿØŸá‚ÄåŸáÿß\n",
    "print(\"*\" * 88)\n",
    "print(\"Total dataset count {}\".format(len(revlist)))\n",
    "\n",
    "_normalizer = hazm.Normalizer()\n",
    "\n",
    "for item in range(0, 50):  # len(revlist)\n",
    "    user_text = revlist[item][6]\n",
    "    if user_text != \"\":\n",
    "        normal_text = _normalizer.normalize(user_text)\n",
    "        tokenized_text = hazm.word_tokenize(normal_text)\n",
    "\n",
    "        # ÿß€åÿ¨ÿßÿØ ÿ™ŸÜÿ≥Ÿàÿ± ÿ≥Ÿá ÿ®ÿπÿØ€å (1,8,300) ÿ®ÿß ŸÖŸÇÿØÿßÿ± ÿµŸÅÿ±\n",
    "        vector_text = np.zeros((1, max_vocab_token, embedding_dim), dtype=K.floatx())\n",
    "\n",
    "        for vocabs in range(0, len(tokenized_text)):\n",
    "            if vocabs >= max_vocab_token:\n",
    "                break  # ÿß⁄Øÿ± ⁄©ÿßŸÖŸÜÿ™ ÿ®€åÿ¥ÿ™ÿ± ÿßÿ≤ 8 ⁄©ŸÑŸÖŸá ÿ®ÿßÿ¥ÿØÿå ŸÅŸÇÿ∑ 8 ⁄©ŸÑŸÖŸá ÿßŸàŸÑ ÿØÿ± ŸÜÿ∏ÿ± ⁄Øÿ±ŸÅÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ\n",
    "            if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "                continue  # ÿß⁄Øÿ± Ÿàÿß⁄òŸá ÿØÿ± fasttext Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ÿØÿå Ÿáÿ± 300 ÿπŸÜÿµÿ± ÿ¢ŸÜ Ÿà⁄©ÿ™Ÿàÿ± ÿµŸÅÿ± ÿ®ÿßŸÇ€å ŸÖ€å‚ÄåŸÖÿßŸÜÿØ\n",
    "            vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "\n",
    "        result = LSTM_model.predict(vector_text, verbose='0', workers=10, use_multiprocessing=True, max_queue_size=100)\n",
    "        \n",
    "        pos_percent = str(int(result[0][1] * 100))\n",
    "        neg_percent = str(int(result[0][0] * 100))\n",
    "\n",
    "        # ÿ™ÿπ€å€åŸÜ ÿßÿ≠ÿ≥ÿßÿ≥ Ÿà ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸÜ ÿ®Ÿá revlist\n",
    "        sentiment = \"positive\" if result[0][1] > result[0][0] else \"negative\"\n",
    "        revlist[item].append(sentiment)\n",
    "\n",
    "# ÿ™ÿ®ÿØ€åŸÑ revlist ÿ®Ÿá DataFrame\n",
    "columns = ['Index', 'Top25', 'Value', 'Date', 'Keyword', 'Get Type', 'Comment', 'Sentiment']\n",
    "results_df = pd.DataFrame(revlist, columns=columns)\n",
    "\n",
    "# ÿ∞ÿÆ€åÿ±Ÿá DataFrame ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© ŸÅÿß€åŸÑ CSV\n",
    "results_df.to_csv('results_with_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Results saved to results_with_sentiment.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
