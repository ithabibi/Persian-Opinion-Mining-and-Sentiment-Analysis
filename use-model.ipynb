{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/blob/main/use-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMNPBU2OwSBw"
   },
   "source": [
    "# Persian Sentiment Analysis With Fasttext language Model and LSTM neural network\n",
    "### Persian sentiment analysis step by step guide\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "so there are 3 steps we going through with each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Load fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pybind11==2.11.1\n",
    "!pip install fasttext==0.9.2 \n",
    "\n",
    "#!pip install keras==2.14.0\n",
    "!pip install tensorflow==2.12.0 #For Deep Learning\n",
    "!pip install keras==2.12.0 #A wrapper for TensorFlow for simplicity\n",
    "\n",
    "!pip install hazm==0.7.0\n",
    "!pip install pandas==1.5.3\n",
    "!pip install numpy==1.23\n",
    "\n",
    "import pandas\n",
    "import random\n",
    "import numpy as np\n",
    "import hazm\n",
    "import keras.backend as K\n",
    "import fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and unzip ELM\n",
    "!rm -rf /content/cc.fa.300.bin.gz\n",
    "!rm -rf /content/cc.fa.300.bin\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
    "!gunzip /content/cc.fa.300.bin.gz\n",
    "\n",
    "%time\n",
    "fasttext_model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Load LSTM learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and unzip learned model\n",
    "!rm -rf /content/learned-query-sentiment-fasttext.model.zip\n",
    "!rm -rf /content/learned-query-sentiment-fasttext.model\n",
    "\n",
    "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/learned-query-sentiment-fasttext.model.zip\n",
    "!unzip /content/learned-query-sentiment-fasttext.model.zip\n",
    "embedding_dim = 300 #@param {type:\"integer\"} #The number 300, is the dimensions of the model\n",
    "max_vocab_token = 8 #@param {type:\"integer\"}\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "#del model  # deletes the existing model\n",
    "\n",
    "LSTM_model = load_model('/content/learned-query-sentiment-fasttext.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 enter persian text and booooom!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title using model\n",
    "user_text = \"ÙØ¹Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø¢Ù‡Ù†Ú¯ Ù¾ÛŒØ´ÙˆØ§Ø²\" #@param {type:\"string\"}\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "if not user_text==\"\":\n",
    "  normal_text = _normalizer.normalize(user_text)\n",
    "  tokenized_text = hazm.word_tokenize(normal_text)\n",
    "  \n",
    "  # create and Prepare three dimension tensor (1,8,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "  for vocabs in range(0,len(tokenized_text)):\n",
    "    if vocabs >= max_vocab_token:\n",
    "      break # If the comment is more than 8 words, only the first 8 words will be considered\n",
    "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "    \n",
    "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "\n",
    "  # print(vector_text.shape)\n",
    "  # print(vector_text)\n",
    "  result = LSTM_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
    "  pos_percent = str(int(result[0][1]*100))+\" % ğŸ˜\"\n",
    "  neg_percent = str(int(result[0][0]*100))+\" % ğŸ¤•\"\n",
    "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "else:\n",
    "  print(\"Please enter your text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1 enter batch persian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title laod test Dataset\n",
    "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/related-query-whit-lexion.csv\n",
    "\n",
    "# load and read sentiment_tagged dataset.csv file in tØ§e path ./content/ in google colab. \n",
    "# this dataset include three element: Query,Score,Suggestion. Query is feature and Suggestion is label.\n",
    "csv_dataset = pandas.read_csv(\"/content/related-query-whit-lexion.csv\")\n",
    "\n",
    "def CleanPersianText(text):\n",
    "  _normalizer = hazm.Normalizer()\n",
    "  text = _normalizer.normalize(text)\n",
    "  return text\n",
    "\n",
    "# Cleansing the dataset and creating a new list with two elements: \"Query\" and \"suggestion\"filde. (but without the third element: \"Score\")\n",
    "# The new list is created by the zip Query --> x= zip(csv_dataset['Query'],csv_dataset['Suggestion'])\n",
    "# valu of suggestion is 1,2,3 or positive,negative,neutral\n",
    "revlist = list(map(lambda x: [CleanPersianText(x[0]),\"1\",\"2\"],zip(csv_dataset['Query'],csv_dataset['Suggestion'],csv_dataset['Score'])))\n",
    "\n",
    "# print number of element exist in positive, neutral, negative, revlist list \n",
    "print(\"*\" * 88)\n",
    "print(\"Total dataset count {}\".format(len(revlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Result\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "for item in range(0,100): #len(revlist)\n",
    "  user_text = revlist[item][0]\n",
    "  if not user_text==\"\":\n",
    "    normal_text = _normalizer.normalize(user_text)\n",
    "    tokenized_text = hazm.word_tokenize(normal_text)\n",
    "    # create and Prepare three dimension tensor (1,8,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "    vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "    for vocabs in range(0,len(tokenized_text)):\n",
    "      if vocabs >= max_vocab_token:\n",
    "        break # If the comment is more than 8 words, only the first 8 words will be considered\n",
    "      if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "        continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "      vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "    \n",
    "    result = LSTM_model.predict(vector_text, verbose='0',workers=10,use_multiprocessing=True,max_queue_size=100) # the result has two element: [0][1] and [0][0]\n",
    "    pos_percent = str(int(result[0][1]*100))\n",
    "    neg_percent = str(int(result[0][0]*100))\n",
    "    #display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "    print(str(item) + \": \" + pos_percent +\"%ğŸ˜\" +\" \" + neg_percent +\"%ğŸ¤•\" + \" \" + revlist[item][0] , \"\\n\")\n",
    "  else:\n",
    "    print(\"Please enter your text\")\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙÛŒÙ„Ø¯ Ø§Ø­Ø³Ø§Ø³ Ø¨Ù‡ Ø±Ù„ÛŒØªØ¯ Ú©ÙˆØ¦Ø±ÛŒ Ù‡Ø§ Ø¨Ø§ Ø±ÙˆØ´ Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ pandas Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ÛŒ\n",
    "import numpy as np  # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ numpy Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ Ø¢Ø±Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¹Ø¯Ø¯ÛŒ\n",
    "import hazm  # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ hazm Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ\n",
    "from keras import backend as K  # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† backend Ø§Ø² Keras Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± DataFrame\n",
    "!wget -q https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/merged_all_operator-data.from2009to2023B.csv\n",
    "csv_dataset = pd.read_csv(\"/content/merged_all_operator-data.from2009to2023B.csv\")  # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ CSV Ø¨Ù‡ DataFrame\n",
    "\n",
    "# ØªØ§Ø¨Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
    "def CleanPersianText(text):\n",
    "    _normalizer = hazm.Normalizer()  # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
    "    text = _normalizer.normalize(text)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†\n",
    "    return text  # Ø¨Ø§Ø²Ú¯Ø´Øª Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡\n",
    "\n",
    "# Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒØ³Øª revlist Ø´Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡\n",
    "revlist = list(map(lambda x: [x[0], x[1], x[2], x[3], x[4], x[5], CleanPersianText(x[6])], \n",
    "                   zip(csv_dataset['index'], csv_dataset['top25'], csv_dataset['value'], \n",
    "                       csv_dataset['date'], csv_dataset['keyword'], csv_dataset['get_type'], \n",
    "                       csv_dataset['Query'])))  # ØªØ±Ú©ÛŒØ¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "print(\"*\" * 88)  # Ú†Ø§Ù¾ Ø®Ø·ÛŒ Ø§Ø² Ø³ØªØ§Ø±Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ø®Ø´â€ŒÙ‡Ø§\n",
    "print(\"Total dataset count {}\".format(len(revlist)))  # Ù†Ù…Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "\n",
    "_normalizer = hazm.Normalizer()  # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†\n",
    "batch_size = 1000  # ØªØ¹ÛŒÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø³ØªÙ‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´\n",
    "results = []  # Ù„ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "i=1\n",
    "# Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ\n",
    "for start in range(0, len(revlist), batch_size):  # ØªÚ©Ø±Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ø³ØªÙ‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "    end = min(start + batch_size, len(revlist))  # ØªØ¹ÛŒÛŒÙ† Ø§Ù†ØªÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡\n",
    "    batch_items = revlist[start:end]  # Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø³ØªÙ‡ ÙØ¹Ù„ÛŒ\n",
    "#ØªØ§Ø¨Ø¹ Ù…ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ† Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§  Ù†Ø´ÙˆØ¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "    for item in batch_items:  # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± Ø¢ÛŒØªÙ… Ø¯Ø± Ø¯Ø³ØªÙ‡\n",
    "        user_text = item[6]  # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ú©Ø§Ø±Ø¨Ø± Ø§Ø² Ø¢ÛŒØªÙ…\n",
    "        if user_text != \"\":  # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…ØªÙ† Ø®Ø§Ù„ÛŒ Ù†Ø¨Ø§Ø´Ø¯\n",
    "            normal_text = _normalizer.normalize(user_text)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ú©Ø§Ø±Ø¨Ø±\n",
    "            tokenized_text = hazm.word_tokenize(normal_text)  # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡\n",
    "\n",
    "            # Ø§ÛŒØ¬Ø§Ø¯ ØªÙ†Ø³ÙˆØ± Ø³Ù‡ Ø¨Ø¹Ø¯ÛŒ (1,8,300) Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± ØµÙØ±\n",
    "            vector_text = np.zeros((1, max_vocab_token, embedding_dim), dtype=K.floatx())  # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ Ø®Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆÚ©ØªÙˆØ±Ù‡Ø§ÛŒ ÙˆØ§Ú˜Ù‡\n",
    "\n",
    "            for vocabs in range(min(len(tokenized_text), max_vocab_token)):  # ØªÚ©Ø±Ø§Ø± Ø¨Ø± Ø±ÙˆÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ ØªØ§ Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¬Ø§Ø²\n",
    "                if tokenized_text[vocabs] not in fasttext_model.words:  # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ ÙˆØ§Ú˜Ù‡ Ø¯Ø± Ù…Ø¯Ù„ FastText ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯\n",
    "                    continue  # Ø§Ú¯Ø± ÙˆØ§Ú˜Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù‡\n",
    "                vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])  # Ú¯Ø±ÙØªÙ† ÙˆÚ©ØªÙˆØ± ÙˆØ§Ú˜Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù†\n",
    "\n",
    "            result = LSTM_model.predict(vector_text, verbose=0)  # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ LSTM Ùˆ Ú©Ø§Ù‡Ø´ verbosity\n",
    "            \n",
    "            pos_percent = str(int(result[0][1] * 100))  # Ø¯Ø±ØµØ¯ Ø§Ø­Ø³Ø§Ø³ Ù…Ø«Ø¨Øª\n",
    "            neg_percent = str(int(result[0][0] * 100))  # Ø¯Ø±ØµØ¯ Ø§Ø­Ø³Ø§Ø³ Ù…Ù†ÙÛŒ\n",
    "\n",
    "            # ØªØ¹ÛŒÛŒÙ† Ø§Ø­Ø³Ø§Ø³ Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬\n",
    "            sentiment = \"positive\" if result[0][1] > result[0][0] else \"negative\"  # ØªØ¹ÛŒÛŒÙ† Ø§Ø­Ø³Ø§Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§ÛŒØ¬ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ\n",
    "            item.append(sentiment)  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø­Ø³Ø§Ø³ Ø¨Ù‡ Ø¢ÛŒØªÙ…\n",
    "            results.append(item)  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ… Ø¨Ù‡ Ù„ÛŒØ³Øª Ù†ØªØ§ÛŒØ¬\n",
    "            i= i+1\n",
    "            print(i)  \n",
    "\n",
    "    # Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§ Ø­Ø°Ù Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ\n",
    "    del batch_items  # Ø­Ø°Ù Ø¯Ø³ØªÙ‡ ÙØ¹Ù„ÛŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡\n",
    "    K.clear_session()  # Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ Ù…Ø¯Ù„ Keras\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ DataFrame\n",
    "columns = ['Index', 'Top25', 'Value', 'Date', 'Keyword', 'Get Type', 'Comment', 'Sentiment']  # Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§\n",
    "results_df = pd.DataFrame(results, columns=columns)  # Ø³Ø§Ø®Øª DataFrame Ø§Ø² Ù†ØªØ§ÛŒØ¬\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ DataFrame Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© ÙØ§ÛŒÙ„ CSV\n",
    "results_df.to_csv('results_with_sentiment.csv', index=False)  # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ CSV Ø¨Ø¯ÙˆÙ† Ø´Ù…Ø§Ø±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³\n",
    "\n",
    "print(\"Results saved to results_with_sentiment.csv\")  # Ú†Ø§Ù¾ Ù¾ÛŒØ§Ù… Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
