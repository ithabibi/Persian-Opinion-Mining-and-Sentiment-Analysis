{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/blob/main/use-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMNPBU2OwSBw"
   },
   "source": [
    "# Persian Sentiment Analysis With Fasttext language Model and LSTM neural network\n",
    "### Persian sentiment analysis step by step guide\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "so there are 3 steps we going through with each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Load fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pybind11==2.11.1\n",
    "!pip install fasttext==0.9.2 \n",
    "\n",
    "#!pip install keras==2.14.0\n",
    "!pip install tensorflow==2.12.0 #For Deep Learning\n",
    "!pip install keras==2.12.0 #A wrapper for TensorFlow for simplicity\n",
    "\n",
    "!pip install hazm==0.7.0\n",
    "!pip install pandas==1.5.3\n",
    "!pip install numpy==1.23\n",
    "\n",
    "import pandas\n",
    "import random\n",
    "import numpy as np\n",
    "import hazm\n",
    "import keras.backend as K\n",
    "import fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and unzip ELM\n",
    "!rm -rf /content/cc.fa.300.bin.gz\n",
    "!rm -rf /content/cc.fa.300.bin\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
    "!gunzip /content/cc.fa.300.bin.gz\n",
    "\n",
    "%time\n",
    "fasttext_model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Load LSTM learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and unzip learned model\n",
    "!rm -rf /content/learned-query-sentiment-fasttext.model.zip\n",
    "!rm -rf /content/learned-query-sentiment-fasttext.model\n",
    "\n",
    "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/learned-query-sentiment-fasttext.model.zip\n",
    "!unzip /content/learned-query-sentiment-fasttext.model.zip\n",
    "embedding_dim = 300 #@param {type:\"integer\"} #The number 300, is the dimensions of the model\n",
    "max_vocab_token = 8 #@param {type:\"integer\"}\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "#del model  # deletes the existing model\n",
    "\n",
    "LSTM_model = load_model('/content/learned-query-sentiment-fasttext.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 enter persian text and booooom!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title using model\n",
    "user_text = \"فعال سازی آهنگ پیشواز\" #@param {type:\"string\"}\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "if not user_text==\"\":\n",
    "  normal_text = _normalizer.normalize(user_text)\n",
    "  tokenized_text = hazm.word_tokenize(normal_text)\n",
    "  \n",
    "  # create and Prepare three dimension tensor (1,8,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "  vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "  for vocabs in range(0,len(tokenized_text)):\n",
    "    if vocabs >= max_vocab_token:\n",
    "      break # If the comment is more than 8 words, only the first 8 words will be considered\n",
    "    if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "      continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "    \n",
    "    vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "\n",
    "  # print(vector_text.shape)\n",
    "  # print(vector_text)\n",
    "  result = LSTM_model.predict(vector_text) # the result has two element: [0][1] and [0][0]\n",
    "  pos_percent = str(int(result[0][1]*100))+\" % 😍\"\n",
    "  neg_percent = str(int(result[0][0]*100))+\" % 🤕\"\n",
    "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "else:\n",
    "  print(\"Please enter your text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1 enter batch persian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title laod test Dataset\n",
    "!wget https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/related-query-whit-lexion.csv\n",
    "\n",
    "# load and read sentiment_tagged dataset.csv file in tاe path ./content/ in google colab. \n",
    "# this dataset include three element: Query,Score,Suggestion. Query is feature and Suggestion is label.\n",
    "csv_dataset = pandas.read_csv(\"/content/related-query-whit-lexion.csv\")\n",
    "\n",
    "def CleanPersianText(text):\n",
    "  _normalizer = hazm.Normalizer()\n",
    "  text = _normalizer.normalize(text)\n",
    "  return text\n",
    "\n",
    "# Cleansing the dataset and creating a new list with two elements: \"Query\" and \"suggestion\"filde. (but without the third element: \"Score\")\n",
    "# The new list is created by the zip Query --> x= zip(csv_dataset['Query'],csv_dataset['Suggestion'])\n",
    "# valu of suggestion is 1,2,3 or positive,negative,neutral\n",
    "revlist = list(map(lambda x: [CleanPersianText(x[0]),\"1\",\"2\"],zip(csv_dataset['Query'],csv_dataset['Suggestion'],csv_dataset['Score'])))\n",
    "\n",
    "# print number of element exist in positive, neutral, negative, revlist list \n",
    "print(\"*\" * 88)\n",
    "print(\"Total dataset count {}\".format(len(revlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Result\n",
    "from IPython.core.display import display, HTML\n",
    "_normalizer = hazm.Normalizer()\n",
    "for item in range(0,100): #len(revlist)\n",
    "  user_text = revlist[item][0]\n",
    "  if not user_text==\"\":\n",
    "    normal_text = _normalizer.normalize(user_text)\n",
    "    tokenized_text = hazm.word_tokenize(normal_text)\n",
    "    # create and Prepare three dimension tensor (1,8,300) with zero value : (1,number_of_words, dimension_of_fasttext)\n",
    "    vector_text = np.zeros((1,max_vocab_token,embedding_dim),dtype=K.floatx())\n",
    "\n",
    "    for vocabs in range(0,len(tokenized_text)):\n",
    "      if vocabs >= max_vocab_token:\n",
    "        break # If the comment is more than 8 words, only the first 8 words will be considered\n",
    "      if tokenized_text[vocabs] not in fasttext_model.words:\n",
    "        continue # If vocab does not exist in fasttext, every 300 elements of that word's vector remain zero\n",
    "      vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])\n",
    "    \n",
    "    result = LSTM_model.predict(vector_text, verbose='0',workers=10,use_multiprocessing=True,max_queue_size=100) # the result has two element: [0][1] and [0][0]\n",
    "    pos_percent = str(int(result[0][1]*100))\n",
    "    neg_percent = str(int(result[0][0]*100))\n",
    "    #display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWlubG92ZWZhY2UtMDFfMS1sOWQzYzlxMC5wbmc.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img height='64px' width='64px' src='https://images.rawpixel.com/image_png_1000/cHJpdmF0ZS9sci9pbWFnZXMvd2Vic2l0ZS8yMDIyLTEwL3JtNTg2LWNyeWluZ2ZhY2UtMDFfMi1sOWQzYnh0MC5wbmc.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
    "    print(str(item) + \": \" + pos_percent +\"%😍\" +\" \" + neg_percent +\"%🤕\" + \" \" + revlist[item][0] , \"\\n\")\n",
    "  else:\n",
    "    print(\"Please enter your text\")\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اضافه کردن فیلد احساس به رلیتد کوئری ها با روش بهبود مصرف حافظه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # وارد کردن کتابخانه pandas برای کار با داده‌های جدولی\n",
    "import numpy as np  # وارد کردن کتابخانه numpy برای کار با آرایه‌ها و محاسبات عددی\n",
    "import hazm  # وارد کردن کتابخانه hazm برای پردازش متون فارسی\n",
    "from keras import backend as K  # وارد کردن backend از Keras برای مدیریت حافظه\n",
    "\n",
    "# بارگذاری داده‌ها از اینترنت و ذخیره در DataFrame\n",
    "!wget -q https://raw.githubusercontent.com/ithabibi/Persian-Opinion-Mining-and-Sentiment-Analysis/main/merged_all_operator-data.from2009to2023B.csv\n",
    "csv_dataset = pd.read_csv(\"/content/merged_all_operator-data.from2009to2023B.csv\")  # خواندن فایل CSV به DataFrame\n",
    "\n",
    "# تابعی برای پاکسازی متن فارسی\n",
    "def CleanPersianText(text):\n",
    "    _normalizer = hazm.Normalizer()  # ایجاد یک نرمالایزر برای متن فارسی\n",
    "    text = _normalizer.normalize(text)  # نرمال‌سازی متن\n",
    "    return text  # بازگشت متن نرمال‌شده\n",
    "\n",
    "# ایجاد لیست revlist شامل اطلاعات پردازش‌شده\n",
    "revlist = list(map(lambda x: [x[0], x[1], x[2], x[3], x[4], x[5], CleanPersianText(x[6])], \n",
    "                   zip(csv_dataset['index'], csv_dataset['top25'], csv_dataset['value'], \n",
    "                       csv_dataset['date'], csv_dataset['keyword'], csv_dataset['get_type'], \n",
    "                       csv_dataset['Query'])))  # ترکیب ستون‌های مختلف و پاکسازی متن\n",
    "\n",
    "# نمایش تعداد کل داده‌ها\n",
    "print(\"*\" * 88)  # چاپ خطی از ستاره‌ها برای جدا کردن بخش‌ها\n",
    "print(\"Total dataset count {}\".format(len(revlist)))  # نمایش تعداد کل داده‌ها\n",
    "\n",
    "_normalizer = hazm.Normalizer()  # ایجاد نرمالایزر برای استفاده در پردازش متن\n",
    "batch_size = 1000  # تعیین اندازه دسته برای پردازش\n",
    "results = []  # لیست برای ذخیره نتایج نهایی\n",
    "i=1\n",
    "# پردازش داده‌ها به صورت دسته‌ای\n",
    "for start in range(0, len(revlist), batch_size):  # تکرار برای هر دسته از داده‌ها\n",
    "    end = min(start + batch_size, len(revlist))  # تعیین انتهای دسته\n",
    "    batch_items = revlist[start:end]  # انتخاب دسته فعلی\n",
    "#تابع مین برای اطمینان از این که مقدار محاسبه‌شده از تعداد کل آیتم‌ها  نشود، استفاده می‌شود.\n",
    "    for item in batch_items:  # پردازش هر آیتم در دسته\n",
    "        user_text = item[6]  # استخراج متن کاربر از آیتم\n",
    "        if user_text != \"\":  # بررسی اینکه متن خالی نباشد\n",
    "            normal_text = _normalizer.normalize(user_text)  # نرمال‌سازی متن کاربر\n",
    "            tokenized_text = hazm.word_tokenize(normal_text)  # توکنایز کردن متن نرمال‌شده\n",
    "\n",
    "            # ایجاد تنسور سه بعدی (1,8,300) با مقدار صفر\n",
    "            vector_text = np.zeros((1, max_vocab_token, embedding_dim), dtype=K.floatx())  # ایجاد یک آرایه خالی برای ذخیره وکتورهای واژه\n",
    "\n",
    "            for vocabs in range(min(len(tokenized_text), max_vocab_token)):  # تکرار بر روی توکن‌ها تا حداکثر تعداد مجاز\n",
    "                if tokenized_text[vocabs] not in fasttext_model.words:  # بررسی اینکه واژه در مدل FastText وجود داشته باشد\n",
    "                    continue  # اگر واژه وجود ندارد، ادامه بده\n",
    "                vector_text[0, vocabs, :] = fasttext_model.get_word_vector(tokenized_text[vocabs])  # گرفتن وکتور واژه و ذخیره آن\n",
    "\n",
    "            result = LSTM_model.predict(vector_text, verbose=0)  # پیش‌بینی با مدل LSTM و کاهش verbosity\n",
    "            \n",
    "            pos_percent = str(int(result[0][1] * 100))  # درصد احساس مثبت\n",
    "            neg_percent = str(int(result[0][0] * 100))  # درصد احساس منفی\n",
    "\n",
    "            # تعیین احساس و اضافه کردن به نتایج\n",
    "            sentiment = \"positive\" if result[0][1] > result[0][0] else \"negative\"  # تعیین احساس بر اساس نتایج پیش‌بینی\n",
    "            item.append(sentiment)  # اضافه کردن احساس به آیتم\n",
    "            results.append(item)  # اضافه کردن آیتم به لیست نتایج\n",
    "            i= i+1\n",
    "            print(i)  \n",
    "\n",
    "    # آزادسازی حافظه با حذف متغیرهای غیرضروری\n",
    "    del batch_items  # حذف دسته فعلی از حافظه\n",
    "    K.clear_session()  # آزادسازی حافظه مدل Keras\n",
    "\n",
    "# تبدیل نتایج به DataFrame\n",
    "columns = ['Index', 'Top25', 'Value', 'Date', 'Keyword', 'Get Type', 'Comment', 'Sentiment']  # نام ستون‌ها\n",
    "results_df = pd.DataFrame(results, columns=columns)  # ساخت DataFrame از نتایج\n",
    "\n",
    "# ذخیره DataFrame به عنوان یک فایل CSV\n",
    "results_df.to_csv('results_with_sentiment.csv', index=False)  # ذخیره نتایج در فایل CSV بدون شماره ایندکس\n",
    "\n",
    "print(\"Results saved to results_with_sentiment.csv\")  # چاپ پیام موفقیت در ذخیره‌سازی نتایج\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
